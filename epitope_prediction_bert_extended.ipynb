{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/refine-epitope-deep-learning')"],"metadata":{"id":"GLxgv9OSFqYi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657040393059,"user_tz":180,"elapsed":2873,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"adb769da-18ce-4bbc-8f8e-203b0af8bcaa"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install optuna"],"metadata":{"id":"84QeWKhKKG_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertConfig, BertForPreTraining, BertTokenizerFast, BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', padding=True, truncation=True, max_length=512)\n","\n","#with open(\"clean.txt\", 'r') as fp:\n","#    text = fp.read().split('\\n')"],"metadata":{"id":"KNruyhF5KHVH","executionInfo":{"status":"ok","timestamp":1657040402308,"user_tz":180,"elapsed":1363,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["config = BertConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig\n","    #vocab_size=20000, # default It's an English standard, so you have to modify it to fit the vocab size you made.\n","    # hidden_size=512,\n","    # num_hidden_layers=12,    # layer num\n","    # num_attention_heads=8,    # transformer attention head number\n","    # intermediate_size=3072,   # transformer Dimension size of the feed-forward network within\n","    # hidden_act=\"gelu\",\n","    # hidden_dropout_prob=0.1,\n","    # attention_probs_dropout_prob=0.1,\n","    #max_position_embeddings=512,    # embedding size Specify how many tokens to use as input\n","    # type_vocab_size=2,    # token type Range of ids (BERT is segmentA and segmentB, two types)\n","    # pad_token_id=0,\n","    # position_embedding_type=\"absolute\"\n",")\n","\n","model = BertForPreTraining(config=config)\n","model.num_parameters()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BN16AjGCKHd9","executionInfo":{"status":"ok","timestamp":1657040406082,"user_tz":180,"elapsed":3255,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"e8b56c34-fee1-472c-ec40-b5771ab4da29"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["110106428"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["#bag_senteces = [item for sentence in text for item in sentence.split('.') if item != '']"],"metadata":{"id":"qBcAROWLKIFh","executionInfo":{"status":"ok","timestamp":1657038993361,"user_tz":180,"elapsed":8,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["\n","with open(\"iedb_linear_epitopes.fasta\", 'r') as fp:\n","    text = fp.read().split('\\n')\n","\n","list_positive_negative = []\n","\n","for row in text:\n","  if ('PositiveID' not in row) and ('NegativeID' not in row): \n","    list_positive_negative.append(row)\n","\n","\n","file = open(\"bert_iedb_linear_epitopes.txt\", \"w\") \n","for item in list_positive_negative:\n","  epitope= ''.join([c for c in item if c.isupper()])\n","  no_epitope = item.split(epitope)\n","  file.writelines(\" \".join(no_epitope[0]) + '\\n')\n","  file.writelines(\" \".join(epitope) + '\\n')\n","  file.writelines(\" \".join(no_epitope[1]) + '\\n\\n')\n","\n","file.close()\n"],"metadata":{"id":"G1NUsEy5w8CZ","executionInfo":{"status":"ok","timestamp":1657040433188,"user_tz":180,"elapsed":27125,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from transformers import TextDatasetForNextSentencePrediction\n","from transformers import DataCollatorForLanguageModeling\n","\n","dataset = TextDatasetForNextSentencePrediction(\n","    tokenizer=tokenizer,\n","    file_path='bert_iedb_linear_epitopes.txt',\n","    block_size=512,\n","    overwrite_cache=False,\n","  #  short_seq_probability=0.1,\n","    nsp_probability=0.5,\n",")\n","\n","data_collator = DataCollatorForLanguageModeling(    # We don't have to implement [MASK]! :-)\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6zaJmML6ZGsY","executionInfo":{"status":"ok","timestamp":1657040466949,"user_tz":180,"elapsed":32782,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"627b3540-5c17-41d7-a2d5-40047d3d7ca0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:366: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["import torch\n","\n","MAX_LEN=512\n","\n","ending_sep_token_tensor = torch.tensor([102])\n","\n","for count, sample in enumerate(dataset.examples):\n","    if len(sample['input_ids'])>MAX_LEN:\n","        dataset.examples[count]['input_ids'] = torch.cat((sample['input_ids'][:MAX_LEN-1], ending_sep_token_tensor), 0)\n","        dataset.examples[count]['token_type_ids'] = sample['token_type_ids'][:MAX_LEN]"],"metadata":{"id":"pt00EAzkdRhh","executionInfo":{"status":"ok","timestamp":1657040466951,"user_tz":180,"elapsed":24,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#dataset.examples[0]"],"metadata":{"id":"Otdo1pIHIsuF","executionInfo":{"status":"ok","timestamp":1657039040965,"user_tz":180,"elapsed":66,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["#tokenizer.decode(data_collator(dataset.examples)['input_ids'][0].tolist())"],"metadata":{"id":"M1E4QEKZKIcr","executionInfo":{"status":"ok","timestamp":1657039040966,"user_tz":180,"elapsed":65,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir= \"./model/bert-retrained\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=5,\n","    per_device_train_batch_size= 8,\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    prediction_loss_only=True,\n","    optim=\"adamw_torch\"\n",")\n","\n","\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset\n",")\n","\n","\n","trainer.train()\n","trainer.save_model(\"./model/bertExtendend\")"],"metadata":{"id":"LHA-8bxRKI-9","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1657044368448,"user_tz":180,"elapsed":3901518,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"55e80bb4-bf99-4316-b5b9-7f2f1267dc03"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 49671\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 31045\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='31045' max='31045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [31045/31045 1:04:55, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>7.180500</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>6.496700</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>6.423000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>6.396500</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>6.425100</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>6.419600</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>6.410400</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>6.385100</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>6.408300</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>6.442600</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>6.501600</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>6.524600</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>6.489200</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>6.485300</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>6.515300</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>6.509000</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>6.485900</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>6.437700</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>6.463500</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>6.469000</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>6.490300</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>6.479600</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>6.477200</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>6.499800</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>6.479500</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>6.467200</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>6.476200</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>6.463300</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>6.501200</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>6.499700</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>6.461900</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>6.482400</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>6.438600</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>6.495700</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>6.479100</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>6.489100</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>6.455400</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>6.445800</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>6.482900</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>6.471200</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>6.488900</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>6.476200</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>6.498100</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>6.464100</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>6.504400</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>6.469500</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>6.460800</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>6.504800</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>6.436100</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>6.483800</td>\n","    </tr>\n","    <tr>\n","      <td>25500</td>\n","      <td>6.478500</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>6.487300</td>\n","    </tr>\n","    <tr>\n","      <td>26500</td>\n","      <td>6.446200</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>6.471400</td>\n","    </tr>\n","    <tr>\n","      <td>27500</td>\n","      <td>6.478900</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>6.503400</td>\n","    </tr>\n","    <tr>\n","      <td>28500</td>\n","      <td>6.476700</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>6.469700</td>\n","    </tr>\n","    <tr>\n","      <td>29500</td>\n","      <td>6.462500</td>\n","    </tr>\n","    <tr>\n","      <td>30000</td>\n","      <td>6.474700</td>\n","    </tr>\n","    <tr>\n","      <td>30500</td>\n","      <td>6.430500</td>\n","    </tr>\n","    <tr>\n","      <td>31000</td>\n","      <td>6.484100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./model/bert-retrained/checkpoint-10000\n","Configuration saved in ./model/bert-retrained/checkpoint-10000/config.json\n","Model weights saved in ./model/bert-retrained/checkpoint-10000/pytorch_model.bin\n","Saving model checkpoint to ./model/bert-retrained/checkpoint-20000\n","Configuration saved in ./model/bert-retrained/checkpoint-20000/config.json\n","Model weights saved in ./model/bert-retrained/checkpoint-20000/pytorch_model.bin\n","Saving model checkpoint to ./model/bert-retrained/checkpoint-30000\n","Configuration saved in ./model/bert-retrained/checkpoint-30000/config.json\n","Model weights saved in ./model/bert-retrained/checkpoint-30000/pytorch_model.bin\n","Deleting older checkpoint [model/bert-retrained/checkpoint-10000] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to ./model/bertExtendend\n","Configuration saved in ./model/bertExtendend/config.json\n","Model weights saved in ./model/bertExtendend/pytorch_model.bin\n"]}]},{"cell_type":"markdown","source":["## Loading pre-trained model"],"metadata":{"id":"Zwqrew-4xuwO"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","import torch\n","from transformers import TrainingArguments, Trainer\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import EarlyStoppingCallback"],"metadata":{"id":"gRNiLEODxz2O","executionInfo":{"status":"ok","timestamp":1657044368450,"user_tz":180,"elapsed":20,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Create torch dataset\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels:\n","            item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])"],"metadata":{"id":"OROvnaINyV9t","executionInfo":{"status":"ok","timestamp":1657044368451,"user_tz":180,"elapsed":15,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def preprocess_data(data):\n","\n","    # Preprocess data\n","    X = list(data[\"sequence\"])\n","    y = list(data[\"label\"])\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n","    X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n","    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n","\n","    train_dataset = Dataset(X_train_tokenized, y_train)\n","    val_dataset = Dataset(X_val_tokenized, y_val)\n","    return train_dataset, val_dataset"],"metadata":{"id":"jaQ4eEXxyZ6V","executionInfo":{"status":"ok","timestamp":1657044368452,"user_tz":180,"elapsed":15,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv(\"./input/data_train.csv\")\n","\n","sequence_formatted = []\n","for seq in df_train['sequence'].values:\n","  sequence_formatted.append(\" \".join(seq))\n","\n","data = pd.DataFrame({'sequence':sequence_formatted, 'label':df_train['label'].tolist()})\n","\n","#data = df_train\n","\n","data_op = data[:int(len(data)/2)]\n","\n","\n","# Define pretrained tokenizer and model\n","batch_size=8\n","model_name = \"bert-base-uncased\"\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertForSequenceClassification.from_pretrained(\"./model/bertExtendend\", num_labels=2)\n","\n","train_dataset_op, val_dataset_op = preprocess_data(data_op)\n","train_dataset, val_dataset = preprocess_data(data)\n","\n","# ----- 2. Fine-tune pretrained model -----#\n","# Define Trainer parameters\n","def compute_metrics(p):\n","    \n","    pred, labels = p\n","    pred = np.argmax(pred, axis=1)\n","\n","    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","    recall = recall_score(y_true=labels, y_pred=pred)\n","    precision = precision_score(y_true=labels, y_pred=pred)\n","    f1 = f1_score(y_true=labels, y_pred=pred)\n","\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","# Define Trainer\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-classification\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model='f1',\n","    optim=\"adamw_torch\"\n",")\n","\n","def model_init():\n","    return model\n","\n","trainer = Trainer(\n","    model_init=model_init,\n","    args=args,\n","    train_dataset=train_dataset_op,\n","    eval_dataset=val_dataset_op,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","\n","#best_run = trainer.hyperparameter_search(n_trials=2, direction=\"maximize\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WSnQhBhIydA7","executionInfo":{"status":"ok","timestamp":1657044373315,"user_tz":180,"elapsed":4877,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"31e9cefd-49da-4266-f529-30140f465313"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading configuration file ./model/bertExtendend/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForPreTraining\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file ./model/bertExtendend/pytorch_model.bin\n","Some weights of the model checkpoint at ./model/bertExtendend were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./model/bertExtendend and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"code","source":["for n, v in best_run.hyperparameters.items():\n","    setattr(trainer.args, n, v)\n","\n","trainer.train_dataset=train_dataset\n","trainer.eval_dataset=val_dataset\n","\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"F8273Ad4y0lV","executionInfo":{"status":"error","timestamp":1657044373941,"user_tz":180,"elapsed":659,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"2f662c50-7bce-4ba6-d75e-59dadb72eddf"},"execution_count":15,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-e2bcd62b0bb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'best_run' is not defined"]}]},{"cell_type":"code","source":["# ----- 3. Predict -----#\n","# Load test data\n","#test_data = pd.read_csv(\"test.csv\")\n","test = pd.read_csv(\"./input/data_test.csv\")\n","\n","sequence_formatted = []\n","for seq in test['sequence'].values:\n","  sequence_formatted.append(\" \".join(seq))\n","\n","test_data = pd.DataFrame({'sequence':sequence_formatted, 'label':test['label'].tolist()})\n","\n","\n","X_test = list(test_data[\"sequence\"])\n","X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n","\n","# Create torch dataset\n","test_dataset = Dataset(X_test_tokenized)\n","\n","# Make prediction\n","raw_pred, _, _ = trainer.predict(test_dataset)\n","\n","# Preprocess raw predictions\n","y_pred = np.argmax(raw_pred, axis=1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"k0IG1dEyy55H","executionInfo":{"status":"ok","timestamp":1657044503578,"user_tz":180,"elapsed":4031,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"d96a64ac-92b5-45ab-aea9-157baaeb6006"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 207\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [26/26 00:02]\n","    </div>\n","    "]},"metadata":{}}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.metrics import roc_auc_score\n","\n","print(\"ROC_AUC:\", roc_auc_score(test_data['label'], y_pred))\n","\n","print(classification_report(test_data['label'], y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zGxz65Z8y7FV","executionInfo":{"status":"ok","timestamp":1657044508669,"user_tz":180,"elapsed":316,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"b9fad3c9-c730-436c-abae-e27eee3d4025"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["ROC_AUC: 0.5\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       119\n","           1       0.43      1.00      0.60        88\n","\n","    accuracy                           0.43       207\n","   macro avg       0.21      0.50      0.30       207\n","weighted avg       0.18      0.43      0.25       207\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]}],"metadata":{"colab":{"name":"epitope_prediction_bert_extended.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}