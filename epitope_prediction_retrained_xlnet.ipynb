{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2843,"status":"ok","timestamp":1657237216155,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"},"user_tz":180},"id":"GLxgv9OSFqYi","outputId":"c6516d8d-ee8e-44a6-da3a-6c26171120ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/refine-epitope-deep-learning')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"84QeWKhKKG_S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657237228159,"user_tz":180,"elapsed":12019,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"7ddb907f-c102-41bd-f229-23de37f04076"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.10.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.37)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.6.0)\n","Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.8.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n","Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n","Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n","Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.10.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.2.1)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.7.1)\n","Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (0.5.1)\n","Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n","Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.9.0)\n","Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n","Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.4.1)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n","Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n"]}],"source":["!pip install transformers\n","!pip install optuna"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2106,"status":"ok","timestamp":1657237230259,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"},"user_tz":180},"id":"G1NUsEy5w8CZ"},"outputs":[],"source":["with open(\"iedb_linear_epitopes.fasta\", 'r') as fp:\n","    text = fp.read().split('\\n')\n","\n","list_positive_negative = []\n","\n","for row in text:\n","  if ('PositiveID' not in row) and ('NegativeID' not in row) and (len(row) < 512): \n","    list_positive_negative.append(row)\n","\n","\n","file = open(\"iedb_linear_epitopes_processed.txt\", \"w\") \n","for item in list_positive_negative:\n","  epitope= ''.join([c for c in item if c.isupper()])\n","  no_epitope = item.split(epitope)\n","  file.writelines(\" \".join(no_epitope[0]) + '\\n')\n","  file.writelines(\" \".join(epitope) + '\\n')\n","  file.writelines(\" \".join(no_epitope[1]) + '\\n\\n')\n","\n","file.close()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":325,"status":"error","timestamp":1657237264947,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"},"user_tz":180},"id":"M-ScHGFmjrsF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fd01a053-b8b0-41cc-eb36-1310900ce9fe"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-fa72318994c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXLNetModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXLNetTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXLNetForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXLNetForPreTraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataCollatorForLanguageModeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXLNetConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'XLNetForPreTraining' from 'transformers' (/usr/local/lib/python3.7/dist-packages/transformers/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification, XLNetForPreTraining\n","\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","from transformers import XLNetConfig\n","from transformers import TextDatasetForNextSentencePrediction"]},{"cell_type":"code","source":["config = XLNetConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig\n","    #vocab_size=20000, # \n","    # hidden_size=512,\n","    # num_hidden_layers=12,    # layer num\n","    # num_attention_heads=8,    # transformer attention head number\n","    # intermediate_size=3072,   # transformer \n","    # hidden_act=\"gelu\",\n","    # hidden_dropout_prob=0.1,\n","    # attention_probs_dropout_prob=0.1,\n","    #max_position_embeddings=512    # embedding size \n","    # type_vocab_size=2,    # token type \n","    # pad_token_id=0,\n","    # position_embedding_type=\"absolute\"\n",")"],"metadata":{"id":"tlNVWxhxpcdE","executionInfo":{"status":"ok","timestamp":1657237235073,"user_tz":180,"elapsed":5,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!pip install SentencePiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7C3V2f8YB-Jd","executionInfo":{"status":"ok","timestamp":1657237244086,"user_tz":180,"elapsed":7113,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"03001089-4411-44bf-e18f-26d3b9e8e0f4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: SentencePiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261,"referenced_widgets":["f7420c30eeb742c1a4eb3708bb9902e6","090190fe89d34b68bff58d275337a55a","a82ae1e386ce44ea822f98c14606fbf3","a9c40f4dbaf740abb401536ab94bf3ad","ae8aee53af9645f58904b91281dc061d","00d56f77386349ddbc7cf223a7882943","66d0c1a09e5e4f5491bf5a70a821024e","1f52c161f2ff4375aa5765cb5e8b51aa","eea23ab5e6004021a2f35df3d8bca972","d19ae7a556c348a2a267703a7bd76a4c","23aca4a25f954bd5a3bd9b850124817e","51b5cf30c5024c99b80a9e99bbe749fe","25a63f3dbd804aa89a43ab39db8ab6cb","bc698ff0c7184d5a941d597546a11f68","b041e7b5d50f436bb3331babf45968b9","fbb3637090f34502b64aa9676ef7671c","7f4e7596068f4397894f5c4a52b3e546","22bf8d96949949b5a8af12a2d1a53cb1","dccb39705df0484bae4d7402f8bed832","845a85fb597b4587b1ce620f662e3367","8a94d5e96e664330945e0f237f9f3b9c","793abff060d942129a5cc1fbc683a756"]},"executionInfo":{"elapsed":1713,"status":"error","timestamp":1657237245784,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"},"user_tz":180},"id":"fJkNphXpjrze","outputId":"811b1ea7-56a6-40df-a00a-94f13cd78c80"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/779k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7420c30eeb742c1a4eb3708bb9902e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51b5cf30c5024c99b80a9e99bbe749fe"}},"metadata":{}},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-47711e955860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model = BertForMaskedLM.from_pretrained('bert-base-uncased')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXLNetForPreTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'XLNetForPreTraining' is not defined"]}],"source":["tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n","#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","#model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","model = XLNetForPreTraining(config=config)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3rO8qM2jr2m"},"outputs":[],"source":["#dataset = LineByLineTextDataset(\n","#    tokenizer=tokenizer,\n","#    file_path=\"roberta_iedb_linear_epitopes.txt\",\n","#    block_size=512,\n","#)"]},{"cell_type":"code","source":["dataset = TextDatasetForNextSentencePrediction(\n","    tokenizer=tokenizer,\n","    file_path='iedb_linear_epitopes_processed.txt',\n","    block_size=512,\n","    overwrite_cache=False,\n"," #   short_seq_probability=0.1,\n","    nsp_probability=0.5,\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYXAudSUpw2q","executionInfo":{"status":"ok","timestamp":1657203161995,"user_tz":180,"elapsed":19812,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"48f199d5-bb1a-4f64-d3ff-3a059daf37b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:366: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["\n","for index, example in enumerate(dataset.examples):\n","  if len(example[\"input_ids\"]) > 512:\n","    #print(dataset.examples[index][\"input_ids\"])\n","    dataset.examples[index][\"input_ids\"] = dataset.examples[index][\"input_ids\"][:512]\n","    dataset.examples[index][\"token_type_ids\"] = dataset.examples[index][\"token_type_ids\"][:512]\n","    "],"metadata":{"id":"wbZXcCplSw4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5-vk3HYjr5y"},"outputs":[],"source":["data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Xq3Vm0tjr8w"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"./model/bert-retrained\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=2,\n","    per_device_train_batch_size=8,\n","    save_steps=10000,\n","    save_total_limit=2,\n","    seed=1\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47YDAjfSjr_0","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1657215865423,"user_tz":180,"elapsed":12688667,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"8f50ca01-53c8-4ba5-eb63-7b7744d0f451"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 55350\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 13838\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='13838' max='13838' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13838/13838 3:31:15, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>3.768100</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>3.168700</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>3.154300</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>3.161200</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>3.149200</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>3.144600</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>3.102800</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>3.078200</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>3.070100</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>3.047200</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>3.035400</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>3.023100</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>3.017300</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>2.990700</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>2.944300</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>2.968200</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>2.938800</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>2.921900</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>2.899500</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>2.877100</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>2.882400</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>2.885500</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>2.847900</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>2.851100</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>2.836000</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>2.853000</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>2.853000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./model/bert-retrained/checkpoint-10000\n","Configuration saved in ./model/bert-retrained/checkpoint-10000/config.json\n","Model weights saved in ./model/bert-retrained/checkpoint-10000/pytorch_model.bin\n","Deleting older checkpoint [model/bert-retrained/checkpoint-400] due to args.save_total_limit\n","Deleting older checkpoint [model/bert-retrained/checkpoint-500] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to ./model/bert-retrained\n","Configuration saved in ./model/bert-retrained/config.json\n","Model weights saved in ./model/bert-retrained/pytorch_model.bin\n"]}],"source":["trainer.train()\n","\n","trainer.save_model(\"./model/bert-retrained\")"]},{"cell_type":"markdown","metadata":{"id":"Zwqrew-4xuwO"},"source":["## Loading pre-trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRNiLEODxz2O"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","import torch\n","from transformers import TrainingArguments, Trainer\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import EarlyStoppingCallback"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OROvnaINyV9t"},"outputs":[],"source":["# Create torch dataset\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels:\n","            item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jaQ4eEXxyZ6V"},"outputs":[],"source":["def preprocess_data(data):\n","\n","    # Preprocess data\n","    X = list(data[\"sequence\"])\n","    y = list(data[\"label\"])\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n","    X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n","    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n","\n","    train_dataset = Dataset(X_train_tokenized, y_train)\n","    val_dataset = Dataset(X_val_tokenized, y_val)\n","    return train_dataset, val_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSnQhBhIydA7","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1657216839823,"user_tz":180,"elapsed":974422,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"b15fb00c-38d9-4551-8759-4498d279b06d"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading configuration file ./model/bert-retrained/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForPreTraining\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file ./model/bert-retrained/pytorch_model.bin\n","Some weights of the model checkpoint at ./model/bert-retrained were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./model/bert-retrained and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","\u001b[32m[I 2022-07-07 17:44:34,098]\u001b[0m A new study created in memory with name: no-name-179ffb09-77a4-46e6-b391-51855477e77e\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 126\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 01:41, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.556358</td>\n","      <td>0.674699</td>\n","      <td>0.609756</td>\n","      <td>0.694444</td>\n","      <td>0.649351</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.515365</td>\n","      <td>0.686747</td>\n","      <td>0.625000</td>\n","      <td>0.694444</td>\n","      <td>0.657895</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.509681</td>\n","      <td>0.710843</td>\n","      <td>0.642857</td>\n","      <td>0.750000</td>\n","      <td>0.692308</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-0/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-0/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-0/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-126/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-0/checkpoint-126 (score: 0.6923076923076924).\n","\u001b[32m[I 2022-07-07 17:46:16,190]\u001b[0m Trial 0 finished with value: 2.796008208658811 and parameters: {'learning_rate': 4.251430093740135e-06, 'num_train_epochs': 3, 'seed': 9, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 2.796008208658811.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 126\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 01:42, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.491812</td>\n","      <td>0.722892</td>\n","      <td>0.651163</td>\n","      <td>0.777778</td>\n","      <td>0.708861</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.477467</td>\n","      <td>0.759036</td>\n","      <td>0.666667</td>\n","      <td>0.888889</td>\n","      <td>0.761905</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.485821</td>\n","      <td>0.734940</td>\n","      <td>0.652174</td>\n","      <td>0.833333</td>\n","      <td>0.731707</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-1/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-1/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-1/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-126/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-1/checkpoint-84 (score: 0.761904761904762).\n","\u001b[32m[I 2022-07-07 17:47:59,716]\u001b[0m Trial 1 finished with value: 2.9521543224861273 and parameters: {'learning_rate': 4.110726728789488e-06, 'num_train_epochs': 3, 'seed': 26, 'per_device_train_batch_size': 32}. Best is trial 1 with value: 2.9521543224861273.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 168\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [168/168 02:17, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.498584</td>\n","      <td>0.746988</td>\n","      <td>0.702703</td>\n","      <td>0.722222</td>\n","      <td>0.712329</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.484944</td>\n","      <td>0.734940</td>\n","      <td>0.666667</td>\n","      <td>0.777778</td>\n","      <td>0.717949</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.478159</td>\n","      <td>0.722892</td>\n","      <td>0.651163</td>\n","      <td>0.777778</td>\n","      <td>0.708861</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.480193</td>\n","      <td>0.722892</td>\n","      <td>0.651163</td>\n","      <td>0.777778</td>\n","      <td>0.708861</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-2/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-2/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-2/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-126/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-2/checkpoint-168\n","Configuration saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-168/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-168/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-168/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-168/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-2/checkpoint-84 (score: 0.717948717948718).\n","\u001b[32m[I 2022-07-07 17:50:17,542]\u001b[0m Trial 2 finished with value: 2.860692894234183 and parameters: {'learning_rate': 1.485929175235732e-06, 'num_train_epochs': 4, 'seed': 15, 'per_device_train_batch_size': 4}. Best is trial 1 with value: 2.9521543224861273.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 126\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 01:40, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.456393</td>\n","      <td>0.783133</td>\n","      <td>0.704545</td>\n","      <td>0.861111</td>\n","      <td>0.775000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.397661</td>\n","      <td>0.795181</td>\n","      <td>0.787879</td>\n","      <td>0.722222</td>\n","      <td>0.753623</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.448766</td>\n","      <td>0.819277</td>\n","      <td>0.838710</td>\n","      <td>0.722222</td>\n","      <td>0.776119</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-3/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-3/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-3/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-126/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-3/checkpoint-126 (score: 0.7761194029850746).\n","\u001b[32m[I 2022-07-07 17:51:58,848]\u001b[0m Trial 3 finished with value: 3.156328411060387 and parameters: {'learning_rate': 2.6045418074458484e-05, 'num_train_epochs': 3, 'seed': 32, 'per_device_train_batch_size': 16}. Best is trial 3 with value: 3.156328411060387.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 126\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 01:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.479403</td>\n","      <td>0.807229</td>\n","      <td>0.833333</td>\n","      <td>0.694444</td>\n","      <td>0.757576</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.474250</td>\n","      <td>0.819277</td>\n","      <td>0.838710</td>\n","      <td>0.722222</td>\n","      <td>0.776119</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.483967</td>\n","      <td>0.819277</td>\n","      <td>0.838710</td>\n","      <td>0.722222</td>\n","      <td>0.776119</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-4/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-4/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-4/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-126/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-4/checkpoint-84 (score: 0.7761194029850746).\n","\u001b[32m[I 2022-07-07 17:53:43,140]\u001b[0m Trial 4 finished with value: 3.156328411060387 and parameters: {'learning_rate': 1.202603101410376e-06, 'num_train_epochs': 3, 'seed': 40, 'per_device_train_batch_size': 32}. Best is trial 3 with value: 3.156328411060387.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 168\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [168/168 02:16, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.521453</td>\n","      <td>0.819277</td>\n","      <td>0.862069</td>\n","      <td>0.694444</td>\n","      <td>0.769231</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.560403</td>\n","      <td>0.819277</td>\n","      <td>0.862069</td>\n","      <td>0.694444</td>\n","      <td>0.769231</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.527220</td>\n","      <td>0.855422</td>\n","      <td>0.875000</td>\n","      <td>0.777778</td>\n","      <td>0.823529</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.538366</td>\n","      <td>0.843373</td>\n","      <td>0.870968</td>\n","      <td>0.750000</td>\n","      <td>0.805970</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-5/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-5/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-5/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-126/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-5/checkpoint-168\n","Configuration saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-168/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-168/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-168/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-5/checkpoint-168/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-5/checkpoint-126 (score: 0.823529411764706).\n","\u001b[32m[I 2022-07-07 17:56:00,778]\u001b[0m Trial 5 finished with value: 3.270311385165119 and parameters: {'learning_rate': 2.687521227755561e-06, 'num_train_epochs': 4, 'seed': 11, 'per_device_train_batch_size': 4}. Best is trial 5 with value: 3.270311385165119.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 42\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [42/42 00:34, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.562076</td>\n","      <td>0.831325</td>\n","      <td>0.843750</td>\n","      <td>0.750000</td>\n","      <td>0.794118</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-6/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-42/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-6/checkpoint-42 (score: 0.7941176470588235).\n","\u001b[32m[I 2022-07-07 17:56:36,273]\u001b[0m Trial 6 finished with value: 3.2191929482636428 and parameters: {'learning_rate': 3.1134727154555828e-06, 'num_train_epochs': 1, 'seed': 34, 'per_device_train_batch_size': 4}. Best is trial 5 with value: 3.270311385165119.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 168\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [168/168 02:17, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.775523</td>\n","      <td>0.819277</td>\n","      <td>0.888889</td>\n","      <td>0.666667</td>\n","      <td>0.761905</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.593467</td>\n","      <td>0.867470</td>\n","      <td>0.857143</td>\n","      <td>0.833333</td>\n","      <td>0.845070</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.657451</td>\n","      <td>0.843373</td>\n","      <td>0.848485</td>\n","      <td>0.777778</td>\n","      <td>0.811594</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.655083</td>\n","      <td>0.855422</td>\n","      <td>0.875000</td>\n","      <td>0.777778</td>\n","      <td>0.823529</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-7/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-7/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-7/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-126/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-7/checkpoint-168\n","Configuration saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-168/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-168/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-168/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-7/checkpoint-168/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-7/checkpoint-84 (score: 0.8450704225352113).\n","\u001b[32m[I 2022-07-07 17:58:54,096]\u001b[0m Trial 7 finished with value: 3.3317288762894717 and parameters: {'learning_rate': 7.241309216553027e-06, 'num_train_epochs': 4, 'seed': 2, 'per_device_train_batch_size': 64}. Best is trial 7 with value: 3.3317288762894717.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 42\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [42/42 00:33, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.374573</td>\n","      <td>0.879518</td>\n","      <td>0.882353</td>\n","      <td>0.833333</td>\n","      <td>0.857143</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-8/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-42/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-8/checkpoint-42 (score: 0.8571428571428571).\n","\u001b[32m[I 2022-07-07 17:59:28,594]\u001b[0m Trial 8 finished with value: 3.4523472039418177 and parameters: {'learning_rate': 3.9923381654127413e-05, 'num_train_epochs': 1, 'seed': 15, 'per_device_train_batch_size': 32}. Best is trial 8 with value: 3.4523472039418177.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 84\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [84/84 01:09, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.484543</td>\n","      <td>0.879518</td>\n","      <td>0.861111</td>\n","      <td>0.861111</td>\n","      <td>0.861111</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.653017</td>\n","      <td>0.843373</td>\n","      <td>0.870968</td>\n","      <td>0.750000</td>\n","      <td>0.805970</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-9/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-9/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-9/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-9/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-9/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-9/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-9/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-9/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-9/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-9/checkpoint-84/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-9/checkpoint-42 (score: 0.8611111111111112).\n","\u001b[32m[I 2022-07-07 18:00:38,937]\u001b[0m Trial 9 finished with value: 3.270311385165119 and parameters: {'learning_rate': 1.8240499751878323e-05, 'num_train_epochs': 2, 'seed': 31, 'per_device_train_batch_size': 32}. Best is trial 8 with value: 3.4523472039418177.\u001b[0m\n"]}],"source":["from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, BertForSequenceClassification\n","\n","df_train = pd.read_csv(\"./input/data_train.csv\")\n","\n","sequence_formatted = []\n","for seq in df_train['sequence'].values:\n","  sequence_formatted.append(\" \".join(seq))\n","\n","data = pd.DataFrame({'sequence':sequence_formatted, 'label':df_train['label'].tolist()})\n","\n","#data = df_train\n","\n","data_op = data[:int(len(data)/2)]\n","\n","\n","# Define pretrained tokenizer and model\n","batch_size=8\n","model_name = 'bert-base-uncased'\n","\n","\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","model = BertForSequenceClassification.from_pretrained(\"./model/bert-retrained\", num_labels=2)\n","\n","\n","\n","train_dataset_op, val_dataset_op = preprocess_data(data_op)\n","train_dataset, val_dataset = preprocess_data(data)\n","\n","# ----- 2. Fine-tune pretrained model -----#\n","# Define Trainer parameters\n","def compute_metrics(p):\n","    \n","    pred, labels = p\n","    pred = np.argmax(pred, axis=1)\n","\n","    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","    recall = recall_score(y_true=labels, y_pred=pred)\n","    precision = precision_score(y_true=labels, y_pred=pred)\n","    f1 = f1_score(y_true=labels, y_pred=pred)\n","\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","# Define Trainer\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-classification\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model='f1',\n","    optim=\"adamw_torch\"\n",")\n","\n","def model_init():\n","    return model\n","\n","trainer = Trainer(\n","    model_init=model_init,\n","    args=args,\n","    train_dataset=train_dataset_op,\n","    eval_dataset=val_dataset_op,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","\n","best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F8273Ad4y0lV","colab":{"base_uri":"https://localhost:8080/","height":497},"executionInfo":{"status":"ok","timestamp":1657216909342,"user_tz":180,"elapsed":69553,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"fa6157cd-3517-4407-9362-47dec6145610"},"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 660\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 83\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='83' max='83' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [83/83 01:05, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.275766</td>\n","      <td>0.891566</td>\n","      <td>0.847458</td>\n","      <td>0.847458</td>\n","      <td>0.847458</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/checkpoint-83\n","Configuration saved in bert-base-uncased-finetuned-classification/checkpoint-83/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/checkpoint-83/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/checkpoint-83/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/checkpoint-83/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/checkpoint-83 (score: 0.847457627118644).\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=83, training_loss=0.3222634189100151, metrics={'train_runtime': 66.086, 'train_samples_per_second': 9.987, 'train_steps_per_second': 1.256, 'total_flos': 148215801927600.0, 'train_loss': 0.3222634189100151, 'epoch': 1.0})"]},"metadata":{},"execution_count":17}],"source":["for n, v in best_run.hyperparameters.items():\n","    setattr(trainer.args, n, v)\n","\n","trainer.train_dataset=train_dataset\n","trainer.eval_dataset=val_dataset\n","\n","trainer.train()"]},{"cell_type":"code","source":["# Load trained model\n","#model_path = \"bert-base-uncased-finetuned-classification/checkpoint-166\"\n","#model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n","\n","# Define test trainer\n","#trainer = Trainer(model)"],"metadata":{"id":"M3PDXL4YcI6k"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0IG1dEyy55H","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1657216910859,"user_tz":180,"elapsed":1537,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"05c27518-a996-4833-dd6d-bc8663bea14f"},"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 207\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [26/26 00:03]\n","    </div>\n","    "]},"metadata":{}}],"source":["# ----- 3. Predict -----#\n","# Load test data\n","#test_data = pd.read_csv(\"test.csv\")\n","test = pd.read_csv(\"./input/data_test.csv\")\n","\n","sequence_formatted = []\n","for seq in test['sequence'].values:\n","  sequence_formatted.append(\" \".join(seq))\n","\n","test_data = pd.DataFrame({'sequence':sequence_formatted, 'label':test['label'].tolist()})\n","\n","\n","X_test = list(test_data[\"sequence\"])\n","X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n","\n","# Create torch dataset\n","test_dataset = Dataset(X_test_tokenized)\n","\n","# Make prediction\n","raw_pred, _, _ = trainer.predict(test_dataset)\n","\n","# Preprocess raw predictions\n","y_pred = np.argmax(raw_pred, axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGxz65Z8y7FV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657216910860,"user_tz":180,"elapsed":35,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"36066238-b9e3-4adf-cde5-6351aec4078c"},"outputs":[{"output_type":"stream","name":"stdout","text":["ROC_AUC: 0.8883212375859435\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.92      0.91       119\n","           1       0.89      0.85      0.87        88\n","\n","    accuracy                           0.89       207\n","   macro avg       0.89      0.89      0.89       207\n","weighted avg       0.89      0.89      0.89       207\n","\n"]}],"source":["from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.metrics import roc_auc_score\n","\n","print(\"ROC_AUC:\", roc_auc_score(test_data['label'], y_pred))\n","\n","print(classification_report(test_data['label'], y_pred))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"epitope_prediction_retrained_xlnet.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f7420c30eeb742c1a4eb3708bb9902e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_090190fe89d34b68bff58d275337a55a","IPY_MODEL_a82ae1e386ce44ea822f98c14606fbf3","IPY_MODEL_a9c40f4dbaf740abb401536ab94bf3ad"],"layout":"IPY_MODEL_ae8aee53af9645f58904b91281dc061d"}},"090190fe89d34b68bff58d275337a55a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00d56f77386349ddbc7cf223a7882943","placeholder":"​","style":"IPY_MODEL_66d0c1a09e5e4f5491bf5a70a821024e","value":"Downloading: 100%"}},"a82ae1e386ce44ea822f98c14606fbf3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f52c161f2ff4375aa5765cb5e8b51aa","max":798011,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eea23ab5e6004021a2f35df3d8bca972","value":798011}},"a9c40f4dbaf740abb401536ab94bf3ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19ae7a556c348a2a267703a7bd76a4c","placeholder":"​","style":"IPY_MODEL_23aca4a25f954bd5a3bd9b850124817e","value":" 779k/779k [00:00&lt;00:00, 1.13MB/s]"}},"ae8aee53af9645f58904b91281dc061d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00d56f77386349ddbc7cf223a7882943":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66d0c1a09e5e4f5491bf5a70a821024e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f52c161f2ff4375aa5765cb5e8b51aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eea23ab5e6004021a2f35df3d8bca972":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d19ae7a556c348a2a267703a7bd76a4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23aca4a25f954bd5a3bd9b850124817e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51b5cf30c5024c99b80a9e99bbe749fe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_25a63f3dbd804aa89a43ab39db8ab6cb","IPY_MODEL_bc698ff0c7184d5a941d597546a11f68","IPY_MODEL_b041e7b5d50f436bb3331babf45968b9"],"layout":"IPY_MODEL_fbb3637090f34502b64aa9676ef7671c"}},"25a63f3dbd804aa89a43ab39db8ab6cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f4e7596068f4397894f5c4a52b3e546","placeholder":"​","style":"IPY_MODEL_22bf8d96949949b5a8af12a2d1a53cb1","value":"Downloading: 100%"}},"bc698ff0c7184d5a941d597546a11f68":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dccb39705df0484bae4d7402f8bed832","max":760,"min":0,"orientation":"horizontal","style":"IPY_MODEL_845a85fb597b4587b1ce620f662e3367","value":760}},"b041e7b5d50f436bb3331babf45968b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a94d5e96e664330945e0f237f9f3b9c","placeholder":"​","style":"IPY_MODEL_793abff060d942129a5cc1fbc683a756","value":" 760/760 [00:00&lt;00:00, 13.4kB/s]"}},"fbb3637090f34502b64aa9676ef7671c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f4e7596068f4397894f5c4a52b3e546":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22bf8d96949949b5a8af12a2d1a53cb1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dccb39705df0484bae4d7402f8bed832":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"845a85fb597b4587b1ce620f662e3367":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a94d5e96e664330945e0f237f9f3b9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"793abff060d942129a5cc1fbc683a756":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}