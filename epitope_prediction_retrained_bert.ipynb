{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131853,"status":"ok","timestamp":1658242247872,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"},"user_tz":180},"id":"GLxgv9OSFqYi","outputId":"d3fa5073-2717-4674-a6dc-61d7f06ea263"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/refine-epitope-deep-learning')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84QeWKhKKG_S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658242263925,"user_tz":180,"elapsed":16087,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"978cabd9-ad5a-46be-e775-07923123578a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 7.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 12.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 76.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 68.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting optuna\n","  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n","\u001b[K     |████████████████████████████████| 308 kB 8.5 MB/s \n","\u001b[?25hCollecting alembic\n","  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 32.8 MB/s \n","\u001b[?25hCollecting colorlog\n","  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n","Collecting cliff\n","  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 11.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.39)\n","Collecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.12.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n","Collecting Mako\n","  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 8.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.8.0)\n","Collecting autopage>=0.4.0\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n","Collecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.9.0-py2.py3-none-any.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 79.1 MB/s \n","\u001b[?25hCollecting cmd2>=1.0.0\n","  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n","\u001b[K     |████████████████████████████████| 147 kB 76.1 MB/s \n","\u001b[?25hCollecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 5.9 MB/s \n","\u001b[?25hCollecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n","Building wheels for collected packages: pyperclip\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=382f9cd9f2f6050279c15c2ce6a0587f50efe2037de0e512788bceaad7a0b88b\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built pyperclip\n","Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n","Successfully installed Mako-1.2.1 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.6.0 optuna-2.10.1 pbr-5.9.0 pyperclip-1.8.2 stevedore-3.5.0\n"]}],"source":["!pip install transformers\n","!pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1NUsEy5w8CZ"},"outputs":[],"source":["with open(\"iedb_linear_epitopes.fasta\", 'r') as fp:\n","    text = fp.read().split('\\n')\n","\n","list_positive_negative = []\n","\n","for row in text:\n","  if ('PositiveID' not in row) and ('NegativeID' not in row) and (len(row) < 512): \n","    list_positive_negative.append(row)\n","\n","\n","file = open(\"roberta_iedb_linear_epitopes.txt\", \"w\") \n","for item in list_positive_negative:\n","  epitope= ''.join([c for c in item if c.isupper()])\n","  no_epitope = item.split(epitope)\n","  file.writelines(\" \".join(no_epitope[0]) + '\\n')\n","  file.writelines(\" \".join(epitope) + '\\n')\n","  file.writelines(\" \".join(no_epitope[1]) + '\\n\\n')\n","\n","file.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-ScHGFmjrsF"},"outputs":[],"source":["from transformers import BertTokenizer, BertForMaskedLM, BertForPreTraining, BertForSequenceClassification\n","from transformers import LineByLineTextDataset\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","from transformers import BertConfig, BertForPreTraining\n","from transformers import TextDatasetForNextSentencePrediction"]},{"cell_type":"code","source":["config = BertConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig\n","    #vocab_size=20000, # \n","    # hidden_size=512,\n","    # num_hidden_layers=12,    # layer num\n","    # num_attention_heads=8,    # transformer attention head number\n","    # intermediate_size=3072,   # transformer \n","    # hidden_act=\"gelu\",\n","    # hidden_dropout_prob=0.1,\n","    # attention_probs_dropout_prob=0.1,\n","    #max_position_embeddings=512    # embedding size \n","    # type_vocab_size=2,    # token type \n","    # pad_token_id=0,\n","    # position_embedding_type=\"absolute\"\n",")"],"metadata":{"id":"tlNVWxhxpcdE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["50d3f9c28e894755b8a85b40c6c3e456","44baf4e1c8df4ee3ab1d451e4a5fbff3","22cddb48973e4e739e8680cd591baca7","fc7138a5b9d147329affe4eaa88723cf","aaed905f8c6e45fcbdc309a00bcf174c","3afe65ad955b4ca9b0e9f32b769ded5b","9185c9355b3445389d2d994ffedeb841","2c79928b698344a28a46b359f7cfa0dd","ca3c08d3555d489bb8ff281c297fdb07","e1699166186448868d7ca1b7f7baf787","356b1d7aa4b74d5e9707190b46b138af","ae4dea68fad04ae69cf7d853fb53e9ac","a741cf70d7274149a2286984d1b8598a","3a817c85f38346349bfb5e9020e9e1a1","3c2a9693e0354fecaca091b728368825","4b84a43c0dc140658e9ab787daa1d1bd","64e3da5c0de44e70ac5573083a4e1e7b","64e1ce724bd04ed28190a5462e362884","33c0347e4cb14a4c9d5788e1259d98c6","0a6463cbf3314d54a1e28fe4089126c4","821c74d671944858a314b76b525a8b67","792f7e5af91a448e9bb2967403e05747","90ce6c943b3d419f857afd26aece489d","66ef2810d062447fb7619a581f617336","8313541c57554e39882b9ad6ae8628aa","7a803cf4097f4c6e94a75f3262ba6a87","2be8146b8737460e84c887621c27631d","dda579a2834a4aa5a3cb25d6ea77ffc3","1c9f99bf97c54ebdb670631605341d51","4cd95948ed6544e791a260f43877d857","ca380df6a20741f78dab7875cc357563","f1f9e904ec0941f392342a135e4af621","d36e8dfe80e3491192d140918a513f77"]},"executionInfo":{"elapsed":6667,"status":"ok","timestamp":1658242280117,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"},"user_tz":180},"id":"fJkNphXpjrze","outputId":"927b435b-0b8e-4b9d-9832-380425518ec4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50d3f9c28e894755b8a85b40c6c3e456"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae4dea68fad04ae69cf7d853fb53e9ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90ce6c943b3d419f857afd26aece489d"}},"metadata":{}}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', model_max_length=512, truncation=True)\n","\n","model = BertForPreTraining(config=config)\n","#model = BertForPreTraining.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3rO8qM2jr2m"},"outputs":[],"source":["#dataset = LineByLineTextDataset(\n","#    tokenizer=tokenizer,\n","#    file_path=\"roberta_iedb_linear_epitopes.txt\",\n","#    block_size=512,\n","#)"]},{"cell_type":"code","source":["dataset = TextDatasetForNextSentencePrediction(\n","    tokenizer=tokenizer,\n","    file_path='roberta_iedb_linear_epitopes.txt',\n","    block_size=512,\n","    overwrite_cache=False,\n"," #   short_seq_probability=0.1,\n","    nsp_probability=0.5,\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYXAudSUpw2q","executionInfo":{"status":"ok","timestamp":1658242296197,"user_tz":180,"elapsed":16103,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"71fefa0b-e5ad-468d-8f0d-56ec711e4da3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:366: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["\n","for index, example in enumerate(dataset.examples):\n","  if len(example[\"input_ids\"]) > 512:\n","    #print(dataset.examples[index][\"input_ids\"])\n","    dataset.examples[index][\"input_ids\"] = dataset.examples[index][\"input_ids\"][:512]\n","    dataset.examples[index][\"token_type_ids\"] = dataset.examples[index][\"token_type_ids\"][:512]\n","    "],"metadata":{"id":"wbZXcCplSw4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5-vk3HYjr5y"},"outputs":[],"source":["data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")"]},{"cell_type":"code","source":["for example in dataset.examples[0:1]:\n","    print(example)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGT47hpEpn0K","executionInfo":{"status":"ok","timestamp":1658255221120,"user_tz":180,"elapsed":415,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"b83f3d16-f860-4388-e11e-f9d9aca68cde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([ 101, 1049, 1037, 1055, 1053, 1047, 1054, 1052, 1055, 1053, 1054, 1044,\n","        1043, 1055, 1047, 1061, 1048, 1037, 1056, 1037, 1055, 1056, 1049, 1040,\n","        1044, 1037, 1054, 1044, 1043, 1042, 1048, 1052, 1054, 1044, 1054, 1040,\n","        1056, 1043, 1045, 1048, 1040, 1055, 1045, 1043, 1054, 1042, 1042, 1043,\n","        1043, 1040, 1054, 1043, 1037, 1052, 1047, 1054, 1043, 1055, 1043, 1047,\n","        1040, 1055, 1044, 1044, 1052, 1037, 1054, 1056, 1037, 1044, 1061, 1043,\n","        1055, 1048, 1052, 1053,  102, 1049, 1047, 1042, 1047, 1047, 1056, 1045,\n","        1043, 1037, 1049, 1037, 1048, 1056, 1056, 1049, 1042, 1058, 1037, 1058,\n","        1055, 1037, 1055, 1037, 1058, 1041, 1047, 1050, 1045, 1056, 1058, 1056,\n","        1037, 1055, 1058, 1040, 1052, 1058, 1045, 1040, 1048, 1048, 1053, 1037,\n","        1040, 1043, 1050, 1037, 1048, 1052, 1055, 1037, 1058, 1047, 1048, 1037,\n","        1061, 1055, 1052, 1037, 1055, 1047, 1056, 1042, 1041, 1055, 1061, 1054,\n","        1058, 1049, 1056, 1053, 1058, 1044, 1056, 1050, 1040, 1037, 1056, 1047,\n","        1047, 1058, 1045, 1058, 1047, 1048, 1037, 1040, 1056, 1052, 1053, 1048,\n","        1056, 1040, 1058, 1048, 1050, 1055, 1056, 1058, 1053, 1049, 1052, 1045,\n","        1055, 1058, 1055, 1059, 1043, 1043, 1053, 1058, 1048, 1055, 1056, 1056,\n","        1037, 1047, 1041, 1042, 1041, 1037, 1037, 1037, 1048, 1043, 1061, 1055,\n","        1037, 1055, 1043, 1058, 1050, 1043, 1058, 1055, 1055, 1055, 1053, 1041,\n","        1048, 1058, 1045, 1055, 1037, 1037, 1052, 1047, 1056, 1037, 1043, 1056,\n","        1037, 1052, 1056, 1037, 1043, 1050, 1061, 1055, 1043, 1058, 1058, 1055,\n","        1048, 1058, 1049, 1056, 1048, 1043, 1055,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1]), 'next_sentence_label': tensor(1)}\n"]}]},{"cell_type":"code","source":["#print(data_collator(dataset.examples)['input_ids'][0])"],"metadata":{"id":"-UMUZRk1sJgL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# m a s q k r p s q r h g s k y l a t a s t m d h a r h g f l p r h r d t g i l d s i g r f f g g d r g a p k r g s g k d s h h p a r t a h y g s l p q\n","\n","tokenizer.decode(data_collator(dataset.examples)['input_ids'][0].tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103},"id":"rDH7BA4NsgaB","executionInfo":{"status":"ok","timestamp":1658258214102,"user_tz":180,"elapsed":8538,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"c3b9729d-fbd0-40b7-d62b-5980f726e783"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[CLS] m a s [MASK] k r p s q r h g s [MASK] y l a t a s t m d h a r h g f l p [MASK] h r [MASK] t g i l d s i g r f f g g [MASK] r g a p k r g s g [MASK] d s h h p [MASK] r t a h y g s l p q [SEP] m k [MASK] k k t i g a [MASK] a l t t m f [MASK] a v s a s a v e k [MASK] i t v t a s v d p v i d l l [MASK] a d g n a l p s a v k l a y s p a s k t f e [MASK] y r [MASK] dixon t q v h t n d [MASK] t k k [MASK] i v k [MASK] a d [MASK] p q l [MASK] [MASK] v l n s t v [MASK] m p i s v s w g g q v l s t t a k e f [MASK] a a titanium l g y s a s g v n g [MASK] s s s q e l v i s a a p k t a g t a p t a g n y s g v v s l v m t l g [MASK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":62}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Xq3Vm0tjr8w"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"./model/bert-retrained\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=2,\n","    per_device_train_batch_size=8,\n","    save_steps=10000,\n","    save_total_limit=2,\n","    seed=1\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47YDAjfSjr_0","colab":{"base_uri":"https://localhost:8080/","height":787},"executionInfo":{"status":"ok","timestamp":1658245585914,"user_tz":180,"elapsed":3280281,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"2ade9788-623d-4ad1-eaf0-8f2ca88eb7d8"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 26149\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6538\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6538' max='6538' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6538/6538 54:31, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>3.878200</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>3.253200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>3.239100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>3.233700</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>3.188600</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>3.166800</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>3.094300</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>3.062700</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>3.033400</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>3.027900</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>2.982600</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>2.977600</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>2.975500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to ./model/bert-retrained\n","Configuration saved in ./model/bert-retrained/config.json\n","Model weights saved in ./model/bert-retrained/pytorch_model.bin\n"]}],"source":["trainer.train()\n","\n","trainer.save_model(\"./model/bert-retrained\")"]},{"cell_type":"markdown","metadata":{"id":"Zwqrew-4xuwO"},"source":["## Loading pre-trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRNiLEODxz2O"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","import torch\n","from transformers import TrainingArguments, Trainer\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import EarlyStoppingCallback"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OROvnaINyV9t"},"outputs":[],"source":["# Create torch dataset\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels:\n","            item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jaQ4eEXxyZ6V"},"outputs":[],"source":["def preprocess_data(data):\n","\n","    # Preprocess data\n","    X = list(data[\"sequence\"])\n","    y = list(data[\"label\"])\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n","    X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n","    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n","\n","    train_dataset = Dataset(X_train_tokenized, y_train)\n","    val_dataset = Dataset(X_val_tokenized, y_val)\n","    return train_dataset, val_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSnQhBhIydA7","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1658246439217,"user_tz":180,"elapsed":853313,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"8d5f354a-537e-428f-860c-c2793243df76"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading configuration file ./model/bert-retrained/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForPreTraining\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file ./model/bert-retrained/pytorch_model.bin\n","Some weights of the model checkpoint at ./model/bert-retrained were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./model/bert-retrained and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","\u001b[32m[I 2022-07-19 15:46:30,673]\u001b[0m A new study created in memory with name: no-name-235ac6ab-620a-4c58-aa5f-59080e4d1917\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 84\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [84/84 01:05, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.547364</td>\n","      <td>0.674699</td>\n","      <td>0.736842</td>\n","      <td>0.388889</td>\n","      <td>0.509091</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.508663</td>\n","      <td>0.783133</td>\n","      <td>0.750000</td>\n","      <td>0.750000</td>\n","      <td>0.750000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-0/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-0/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-84/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-0/checkpoint-84 (score: 0.75).\n","\u001b[32m[I 2022-07-19 15:47:37,150]\u001b[0m Trial 0 finished with value: 3.033132530120482 and parameters: {'learning_rate': 2.8432294530739172e-05, 'num_train_epochs': 2, 'seed': 34, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 3.033132530120482.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 84\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [84/84 01:42, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.504875</td>\n","      <td>0.795181</td>\n","      <td>0.756757</td>\n","      <td>0.777778</td>\n","      <td>0.767123</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.500621</td>\n","      <td>0.807229</td>\n","      <td>0.777778</td>\n","      <td>0.777778</td>\n","      <td>0.777778</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-1/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-1/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-84/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-1/checkpoint-84 (score: 0.7777777777777778).\n","\u001b[32m[I 2022-07-19 15:49:20,298]\u001b[0m Trial 1 finished with value: 3.140562248995984 and parameters: {'learning_rate': 1.2129538424847702e-06, 'num_train_epochs': 2, 'seed': 36, 'per_device_train_batch_size': 32}. Best is trial 1 with value: 3.140562248995984.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 84\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [84/84 01:05, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.466362</td>\n","      <td>0.819277</td>\n","      <td>0.783784</td>\n","      <td>0.805556</td>\n","      <td>0.794521</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.465909</td>\n","      <td>0.819277</td>\n","      <td>0.783784</td>\n","      <td>0.805556</td>\n","      <td>0.794521</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-2/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-2/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-84/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-2/checkpoint-42 (score: 0.7945205479452055).\n","\u001b[32m[I 2022-07-19 15:50:26,070]\u001b[0m Trial 2 finished with value: 3.2031369957182796 and parameters: {'learning_rate': 1.6316150285463638e-06, 'num_train_epochs': 2, 'seed': 13, 'per_device_train_batch_size': 4}. Best is trial 2 with value: 3.2031369957182796.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 210\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [210/210 02:15, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.416626</td>\n","      <td>0.831325</td>\n","      <td>0.805556</td>\n","      <td>0.805556</td>\n","      <td>0.805556</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.598228</td>\n","      <td>0.843373</td>\n","      <td>0.744681</td>\n","      <td>0.972222</td>\n","      <td>0.843373</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.845394</td>\n","      <td>0.807229</td>\n","      <td>0.916667</td>\n","      <td>0.611111</td>\n","      <td>0.733333</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.683336</td>\n","      <td>0.807229</td>\n","      <td>0.833333</td>\n","      <td>0.694444</td>\n","      <td>0.757576</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>0.556721</td>\n","      <td>0.855422</td>\n","      <td>0.800000</td>\n","      <td>0.888889</td>\n","      <td>0.842105</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-3/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-3/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-3/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-126/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-3/checkpoint-168\n","Configuration saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-168/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-168/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-168/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-168/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-3/checkpoint-210\n","Configuration saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-210/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-210/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-210/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-210/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-3/checkpoint-84 (score: 0.8433734939759036).\n","\u001b[32m[I 2022-07-19 15:52:42,155]\u001b[0m Trial 3 finished with value: 3.3864158387937717 and parameters: {'learning_rate': 5.467587478594116e-05, 'num_train_epochs': 5, 'seed': 7, 'per_device_train_batch_size': 64}. Best is trial 3 with value: 3.3864158387937717.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 126\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 02:07, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.731075</td>\n","      <td>0.819277</td>\n","      <td>0.783784</td>\n","      <td>0.805556</td>\n","      <td>0.794521</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.669459</td>\n","      <td>0.819277</td>\n","      <td>0.769231</td>\n","      <td>0.833333</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.676048</td>\n","      <td>0.807229</td>\n","      <td>0.763158</td>\n","      <td>0.805556</td>\n","      <td>0.783784</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-4/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-4/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-4/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-126/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-4/checkpoint-84 (score: 0.8).\n","\u001b[32m[I 2022-07-19 15:54:50,383]\u001b[0m Trial 4 finished with value: 3.1597261497388325 and parameters: {'learning_rate': 3.936583869764632e-06, 'num_train_epochs': 3, 'seed': 3, 'per_device_train_batch_size': 8}. Best is trial 3 with value: 3.3864158387937717.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 210\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 42/210 00:16 < 01:10, 2.40 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.687152</td>\n","      <td>0.807229</td>\n","      <td>0.763158</td>\n","      <td>0.805556</td>\n","      <td>0.783784</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","\u001b[32m[I 2022-07-19 15:55:07,611]\u001b[0m Trial 5 pruned. \u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 210\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [210/210 03:05, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.697991</td>\n","      <td>0.831325</td>\n","      <td>0.775000</td>\n","      <td>0.861111</td>\n","      <td>0.815789</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.777893</td>\n","      <td>0.807229</td>\n","      <td>0.777778</td>\n","      <td>0.777778</td>\n","      <td>0.777778</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.599945</td>\n","      <td>0.831325</td>\n","      <td>0.775000</td>\n","      <td>0.861111</td>\n","      <td>0.815789</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.725772</td>\n","      <td>0.807229</td>\n","      <td>0.763158</td>\n","      <td>0.805556</td>\n","      <td>0.783784</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>0.715693</td>\n","      <td>0.807229</td>\n","      <td>0.763158</td>\n","      <td>0.805556</td>\n","      <td>0.783784</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-6/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-6/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-6/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-126/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-6/checkpoint-168\n","Configuration saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-168/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-168/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-168/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-6/checkpoint-168/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","\u001b[32m[I 2022-07-19 15:58:13,528]\u001b[0m Trial 6 pruned. \u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 210\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 42/210 00:16 < 01:08, 2.44 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.725064</td>\n","      <td>0.807229</td>\n","      <td>0.763158</td>\n","      <td>0.805556</td>\n","      <td>0.783784</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","\u001b[32m[I 2022-07-19 15:58:30,371]\u001b[0m Trial 7 pruned. \u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 168\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [168/168 01:50, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.679281</td>\n","      <td>0.855422</td>\n","      <td>0.785714</td>\n","      <td>0.916667</td>\n","      <td>0.846154</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.841690</td>\n","      <td>0.819277</td>\n","      <td>0.838710</td>\n","      <td>0.722222</td>\n","      <td>0.776119</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.812003</td>\n","      <td>0.819277</td>\n","      <td>0.769231</td>\n","      <td>0.833333</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.811638</td>\n","      <td>0.819277</td>\n","      <td>0.769231</td>\n","      <td>0.833333</td>\n","      <td>0.800000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-8/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-8/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-8/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-126/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-8/checkpoint-168\n","Configuration saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-168/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-168/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-168/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-8/checkpoint-168/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-8/checkpoint-42 (score: 0.8461538461538461).\n","\u001b[32m[I 2022-07-19 16:00:21,149]\u001b[0m Trial 8 finished with value: 3.2218412109978374 and parameters: {'learning_rate': 7.813963211738485e-06, 'num_train_epochs': 4, 'seed': 21, 'per_device_train_batch_size': 8}. Best is trial 3 with value: 3.3864158387937717.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 84\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [42/84 00:17 < 00:17, 2.35 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.741790</td>\n","      <td>0.807229</td>\n","      <td>0.763158</td>\n","      <td>0.805556</td>\n","      <td>0.783784</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","\u001b[32m[I 2022-07-19 16:00:38,646]\u001b[0m Trial 9 pruned. \u001b[0m\n"]}],"source":["from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, BertForSequenceClassification\n","\n","df_train = pd.read_csv(\"./input/data_train.csv\")\n","\n","sequence_formatted = []\n","for seq in df_train['sequence'].values:\n","  sequence_formatted.append(\" \".join(seq))\n","\n","data = pd.DataFrame({'sequence':sequence_formatted, 'label':df_train['label'].tolist()})\n","\n","#data = df_train\n","\n","data_op = data[:int(len(data)/2)]\n","\n","\n","# Define pretrained tokenizer and model\n","batch_size=8\n","model_name = 'bert-base-uncased'\n","\n","\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","model = BertForSequenceClassification.from_pretrained(\"./model/bert-retrained\", num_labels=2)\n","\n","\n","\n","train_dataset_op, val_dataset_op = preprocess_data(data_op)\n","train_dataset, val_dataset = preprocess_data(data)\n","\n","# ----- 2. Fine-tune pretrained model -----#\n","# Define Trainer parameters\n","def compute_metrics(p):\n","    \n","    pred, labels = p\n","    pred = np.argmax(pred, axis=1)\n","\n","    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","    recall = recall_score(y_true=labels, y_pred=pred)\n","    precision = precision_score(y_true=labels, y_pred=pred)\n","    f1 = f1_score(y_true=labels, y_pred=pred)\n","\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","# Define Trainer\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-classification\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model='f1',\n","    optim=\"adamw_torch\"\n",")\n","\n","def model_init():\n","    return model\n","\n","trainer = Trainer(\n","    model_init=model_init,\n","    args=args,\n","    train_dataset=train_dataset_op,\n","    eval_dataset=val_dataset_op,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","\n","best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F8273Ad4y0lV","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1658246665290,"user_tz":180,"elapsed":226110,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"a31727b3-5ab3-40b5-9829-4f40ae7a1f2c"},"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 660\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 415\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='415' max='415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [415/415 03:45, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.302826</td>\n","      <td>0.909639</td>\n","      <td>0.879310</td>\n","      <td>0.864407</td>\n","      <td>0.871795</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.324948</td>\n","      <td>0.909639</td>\n","      <td>0.907407</td>\n","      <td>0.830508</td>\n","      <td>0.867257</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.368140</td>\n","      <td>0.909639</td>\n","      <td>0.866667</td>\n","      <td>0.881356</td>\n","      <td>0.873950</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.433276</td>\n","      <td>0.873494</td>\n","      <td>0.787879</td>\n","      <td>0.881356</td>\n","      <td>0.832000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>0.328801</td>\n","      <td>0.921687</td>\n","      <td>0.910714</td>\n","      <td>0.864407</td>\n","      <td>0.886957</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/checkpoint-83\n","Configuration saved in bert-base-uncased-finetuned-classification/checkpoint-83/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/checkpoint-83/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/checkpoint-83/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/checkpoint-83/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/checkpoint-166\n","Configuration saved in bert-base-uncased-finetuned-classification/checkpoint-166/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/checkpoint-166/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/checkpoint-166/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/checkpoint-166/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/checkpoint-249\n","Configuration saved in bert-base-uncased-finetuned-classification/checkpoint-249/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/checkpoint-249/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/checkpoint-249/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/checkpoint-249/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/checkpoint-332\n","Configuration saved in bert-base-uncased-finetuned-classification/checkpoint-332/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/checkpoint-332/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/checkpoint-332/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/checkpoint-332/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/checkpoint-415\n","Configuration saved in bert-base-uncased-finetuned-classification/checkpoint-415/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/checkpoint-415/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/checkpoint-415/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/checkpoint-415/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/checkpoint-415 (score: 0.8869565217391304).\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=415, training_loss=0.316370511342244, metrics={'train_runtime': 226.3049, 'train_samples_per_second': 14.582, 'train_steps_per_second': 1.834, 'total_flos': 813321494330400.0, 'train_loss': 0.316370511342244, 'epoch': 5.0})"]},"metadata":{},"execution_count":17}],"source":["for n, v in best_run.hyperparameters.items():\n","    setattr(trainer.args, n, v)\n","\n","trainer.train_dataset=train_dataset\n","trainer.eval_dataset=val_dataset\n","\n","trainer.train()"]},{"cell_type":"code","source":["# Load trained model\n","model_path = \"bert-base-uncased-finetuned-classification/checkpoint-83\"\n","model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', model_max_length=512, truncation=True)\n","\n","\n","# Define test trainer\n","trainer = Trainer(model)"],"metadata":{"id":"M3PDXL4YcI6k","executionInfo":{"status":"ok","timestamp":1658246672348,"user_tz":180,"elapsed":7080,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"911764f6-5a2d-4cd9-bb31-ae1c13b2c953"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file bert-base-uncased-finetuned-classification/checkpoint-83/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"./model/bert-retrained\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file bert-base-uncased-finetuned-classification/checkpoint-83/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-base-uncased-finetuned-classification/checkpoint-83.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0IG1dEyy55H","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1658246675697,"user_tz":180,"elapsed":3374,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"5fc8d72b-c00f-426b-9305-395cfb2116a6"},"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 207\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [26/26 00:02]\n","    </div>\n","    "]},"metadata":{}}],"source":["import pandas as pd\n","# ----- 3. Predict -----#\n","# Load test data\n","#test_data = pd.read_csv(\"test.csv\")\n","test = pd.read_csv(\"./input/data_test.csv\")\n","\n","sequence_formatted = []\n","for seq in test['sequence'].values:\n","  sequence_formatted.append(\" \".join(seq))\n","\n","test_data = pd.DataFrame({'sequence':sequence_formatted, 'label':test['label'].tolist()})\n","\n","\n","X_test = list(test_data[\"sequence\"])\n","X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n","\n","# Create torch dataset\n","test_dataset = Dataset(X_test_tokenized)\n","\n","# Make prediction\n","raw_pred, _, _ = trainer.predict(test_dataset)\n","\n","# Preprocess raw predictions\n","y_pred = np.argmax(raw_pred, axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGxz65Z8y7FV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658246675700,"user_tz":180,"elapsed":59,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"a1a9ad63-b9f5-465d-f653-95c3d1a6770c"},"outputs":[{"output_type":"stream","name":"stdout","text":["ROC_AUC: 0.8927616501145912\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.90      0.91       119\n","           1       0.87      0.89      0.88        88\n","\n","    accuracy                           0.89       207\n","   macro avg       0.89      0.89      0.89       207\n","weighted avg       0.89      0.89      0.89       207\n","\n"]}],"source":["from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.metrics import roc_auc_score\n","\n","print(\"ROC_AUC:\", roc_auc_score(test_data['label'], y_pred))\n","\n","print(classification_report(test_data['label'], y_pred))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"epitope_prediction_retrained_bert.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"50d3f9c28e894755b8a85b40c6c3e456":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_44baf4e1c8df4ee3ab1d451e4a5fbff3","IPY_MODEL_22cddb48973e4e739e8680cd591baca7","IPY_MODEL_fc7138a5b9d147329affe4eaa88723cf"],"layout":"IPY_MODEL_aaed905f8c6e45fcbdc309a00bcf174c"}},"44baf4e1c8df4ee3ab1d451e4a5fbff3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3afe65ad955b4ca9b0e9f32b769ded5b","placeholder":"​","style":"IPY_MODEL_9185c9355b3445389d2d994ffedeb841","value":"Downloading: 100%"}},"22cddb48973e4e739e8680cd591baca7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c79928b698344a28a46b359f7cfa0dd","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca3c08d3555d489bb8ff281c297fdb07","value":231508}},"fc7138a5b9d147329affe4eaa88723cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1699166186448868d7ca1b7f7baf787","placeholder":"​","style":"IPY_MODEL_356b1d7aa4b74d5e9707190b46b138af","value":" 226k/226k [00:00&lt;00:00, 900kB/s]"}},"aaed905f8c6e45fcbdc309a00bcf174c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3afe65ad955b4ca9b0e9f32b769ded5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9185c9355b3445389d2d994ffedeb841":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c79928b698344a28a46b359f7cfa0dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca3c08d3555d489bb8ff281c297fdb07":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e1699166186448868d7ca1b7f7baf787":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"356b1d7aa4b74d5e9707190b46b138af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae4dea68fad04ae69cf7d853fb53e9ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a741cf70d7274149a2286984d1b8598a","IPY_MODEL_3a817c85f38346349bfb5e9020e9e1a1","IPY_MODEL_3c2a9693e0354fecaca091b728368825"],"layout":"IPY_MODEL_4b84a43c0dc140658e9ab787daa1d1bd"}},"a741cf70d7274149a2286984d1b8598a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64e3da5c0de44e70ac5573083a4e1e7b","placeholder":"​","style":"IPY_MODEL_64e1ce724bd04ed28190a5462e362884","value":"Downloading: 100%"}},"3a817c85f38346349bfb5e9020e9e1a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_33c0347e4cb14a4c9d5788e1259d98c6","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a6463cbf3314d54a1e28fe4089126c4","value":28}},"3c2a9693e0354fecaca091b728368825":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_821c74d671944858a314b76b525a8b67","placeholder":"​","style":"IPY_MODEL_792f7e5af91a448e9bb2967403e05747","value":" 28.0/28.0 [00:00&lt;00:00, 918B/s]"}},"4b84a43c0dc140658e9ab787daa1d1bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64e3da5c0de44e70ac5573083a4e1e7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64e1ce724bd04ed28190a5462e362884":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33c0347e4cb14a4c9d5788e1259d98c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a6463cbf3314d54a1e28fe4089126c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"821c74d671944858a314b76b525a8b67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"792f7e5af91a448e9bb2967403e05747":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90ce6c943b3d419f857afd26aece489d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66ef2810d062447fb7619a581f617336","IPY_MODEL_8313541c57554e39882b9ad6ae8628aa","IPY_MODEL_7a803cf4097f4c6e94a75f3262ba6a87"],"layout":"IPY_MODEL_2be8146b8737460e84c887621c27631d"}},"66ef2810d062447fb7619a581f617336":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dda579a2834a4aa5a3cb25d6ea77ffc3","placeholder":"​","style":"IPY_MODEL_1c9f99bf97c54ebdb670631605341d51","value":"Downloading: 100%"}},"8313541c57554e39882b9ad6ae8628aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cd95948ed6544e791a260f43877d857","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca380df6a20741f78dab7875cc357563","value":570}},"7a803cf4097f4c6e94a75f3262ba6a87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1f9e904ec0941f392342a135e4af621","placeholder":"​","style":"IPY_MODEL_d36e8dfe80e3491192d140918a513f77","value":" 570/570 [00:00&lt;00:00, 11.9kB/s]"}},"2be8146b8737460e84c887621c27631d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dda579a2834a4aa5a3cb25d6ea77ffc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c9f99bf97c54ebdb670631605341d51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cd95948ed6544e791a260f43877d857":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca380df6a20741f78dab7875cc357563":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1f9e904ec0941f392342a135e4af621":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d36e8dfe80e3491192d140918a513f77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}