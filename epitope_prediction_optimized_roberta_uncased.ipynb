{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/refine-epitope-deep-learning')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLxgv9OSFqYi","executionInfo":{"status":"ok","timestamp":1655776289812,"user_tz":180,"elapsed":3489,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"b5388a0c-a41e-43b0-ff23-532cb6466b2d"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install optuna\n","!pip install SentencePiece"],"metadata":{"id":"Eu47NwKQudlO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655776301999,"user_tz":180,"elapsed":12201,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"f6c0fdba-ea69-43ea-fb88-ee7f248e7829"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.10.1)\n","Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.8.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n","Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n","Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.10.1)\n","Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.37)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.4)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.2.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.7.1)\n","Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.4.1)\n","Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n","Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.9.0)\n","Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n","Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (0.5.1)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n","Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: SentencePiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}]},{"cell_type":"markdown","source":["# Preprocess data "],"metadata":{"id":"QEpSHqtISuKr"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","import torch\n","from transformers import TrainingArguments, Trainer\n","from transformers import BertTokenizer, BertForSequenceClassification\n","#from transformers import EarlyStoppingCallback\n","from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification"],"metadata":{"id":"eNWYJ8r-c_w3","executionInfo":{"status":"ok","timestamp":1655776302001,"user_tz":180,"elapsed":13,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Create torch dataset\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels:\n","            item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])"],"metadata":{"id":"aj8NS4koSdbh","executionInfo":{"status":"ok","timestamp":1655776302475,"user_tz":180,"elapsed":485,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def preprocess_data(data):\n","\n","    # Preprocess data\n","    X = list(data[\"sequence\"])\n","    y = list(data[\"label\"])\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n","    X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n","    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n","\n","    train_dataset = Dataset(X_train_tokenized, y_train)\n","    val_dataset = Dataset(X_val_tokenized, y_val)\n","    return train_dataset, val_dataset"],"metadata":{"id":"Q3IrTXnUzpRD","executionInfo":{"status":"ok","timestamp":1655776302476,"user_tz":180,"elapsed":11,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv(\"./input/data_train.csv\")\n","\n","sequence_formatted = []\n","for seq in df_train['sequence'].values:\n","  sequence_formatted.append(\" \".join(seq))\n","\n","data = pd.DataFrame({'sequence':sequence_formatted, 'label':df_train['label'].tolist()})\n","\n","#data = df_train\n","\n","data_op = data[:int(len(data)/5)]\n","\n","\n","# Define pretrained tokenizer and model\n","batch_size=8\n","#model_name = \"cardiffnlp/twitter-roberta-base-emotion\"\n","model_name = 'roberta-base' #'roberta-large' , 'roberta-large-mnli'\n","\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","\n","model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","\n","train_dataset_op, val_dataset_op = preprocess_data(data_op)\n","train_dataset, val_dataset = preprocess_data(data)\n","\n","# ----- 2. Fine-tune pretrained model -----#\n","# Define Trainer parameters\n","def compute_metrics(p):\n","    \n","    pred, labels = p\n","    pred = np.argmax(pred, axis=1)\n","\n","    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","    recall = recall_score(y_true=labels, y_pred=pred)\n","    precision = precision_score(y_true=labels, y_pred=pred)\n","    f1 = f1_score(y_true=labels, y_pred=pred)\n","\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","# Define Trainer\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-classification\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    \n","    #evaluation_strategy ='steps',\n","    #eval_steps = 50, # Evaluation and Save happens every 50 steps\n","    #save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n","    \n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=1,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model='f1',\n","    optim=\"adamw_torch\"\n",")\n","\n","def model_init():\n","    return model\n","\n","trainer = Trainer(\n","    model_init=model_init,\n","    args=args,\n","    train_dataset=train_dataset_op,\n","    eval_dataset=val_dataset_op,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n","  #  callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",")\n","\n","\n","best_run = trainer.hyperparameter_search(n_trials=5, direction=\"maximize\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"61S6rdO_FPMP","executionInfo":{"status":"ok","timestamp":1655776531078,"user_tz":180,"elapsed":228610,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"d9cd2e88-4758-4c23-bc11-81a0463d8a4f"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","\u001b[32m[I 2022-06-21 01:51:45,369]\u001b[0m A new study created in memory with name: no-name-d60787ca-b9ad-4c99-8a1a-74e39b3c95a9\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 132\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 17\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [17/17 00:21, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.583367</td>\n","      <td>0.515152</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Saving model checkpoint to roberta-base-finetuned-classification/run-0/checkpoint-17\n","Configuration saved in roberta-base-finetuned-classification/run-0/checkpoint-17/config.json\n","Model weights saved in roberta-base-finetuned-classification/run-0/checkpoint-17/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/run-0/checkpoint-17/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/run-0/checkpoint-17/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from roberta-base-finetuned-classification/run-0/checkpoint-17 (score: 0.0).\n","\u001b[32m[I 2022-06-21 01:52:07,064]\u001b[0m Trial 0 finished with value: 0.5151515151515151 and parameters: {'learning_rate': 5.7179973095939476e-05, 'num_train_epochs': 1, 'seed': 17, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 0.5151515151515151.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 132\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 17\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [17/17 00:21, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.556034</td>\n","      <td>0.515152</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Saving model checkpoint to roberta-base-finetuned-classification/run-1/checkpoint-17\n","Configuration saved in roberta-base-finetuned-classification/run-1/checkpoint-17/config.json\n","Model weights saved in roberta-base-finetuned-classification/run-1/checkpoint-17/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/run-1/checkpoint-17/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/run-1/checkpoint-17/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from roberta-base-finetuned-classification/run-1/checkpoint-17 (score: 0.0).\n","\u001b[32m[I 2022-06-21 01:52:29,459]\u001b[0m Trial 1 finished with value: 0.5151515151515151 and parameters: {'learning_rate': 2.875699826973366e-06, 'num_train_epochs': 1, 'seed': 9, 'per_device_train_batch_size': 64}. Best is trial 0 with value: 0.5151515151515151.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 132\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 68\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [68/68 01:14, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.539043</td>\n","      <td>0.515152</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.561637</td>\n","      <td>0.515152</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.558612</td>\n","      <td>0.515152</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.559338</td>\n","      <td>0.515152</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Saving model checkpoint to roberta-base-finetuned-classification/run-2/checkpoint-17\n","Configuration saved in roberta-base-finetuned-classification/run-2/checkpoint-17/config.json\n","Model weights saved in roberta-base-finetuned-classification/run-2/checkpoint-17/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/run-2/checkpoint-17/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/run-2/checkpoint-17/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Saving model checkpoint to roberta-base-finetuned-classification/run-2/checkpoint-34\n","Configuration saved in roberta-base-finetuned-classification/run-2/checkpoint-34/config.json\n","Model weights saved in roberta-base-finetuned-classification/run-2/checkpoint-34/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/run-2/checkpoint-34/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/run-2/checkpoint-34/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Saving model checkpoint to roberta-base-finetuned-classification/run-2/checkpoint-51\n","Configuration saved in roberta-base-finetuned-classification/run-2/checkpoint-51/config.json\n","Model weights saved in roberta-base-finetuned-classification/run-2/checkpoint-51/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/run-2/checkpoint-51/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/run-2/checkpoint-51/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Saving model checkpoint to roberta-base-finetuned-classification/run-2/checkpoint-68\n","Configuration saved in roberta-base-finetuned-classification/run-2/checkpoint-68/config.json\n","Model weights saved in roberta-base-finetuned-classification/run-2/checkpoint-68/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/run-2/checkpoint-68/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/run-2/checkpoint-68/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from roberta-base-finetuned-classification/run-2/checkpoint-17 (score: 0.0).\n","\u001b[32m[I 2022-06-21 01:53:44,316]\u001b[0m Trial 2 finished with value: 0.5151515151515151 and parameters: {'learning_rate': 1.0590489982178075e-06, 'num_train_epochs': 4, 'seed': 32, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 0.5151515151515151.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 132\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 17\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [17/17 00:16, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.407955</td>\n","      <td>0.848485</td>\n","      <td>0.789474</td>\n","      <td>0.937500</td>\n","      <td>0.857143</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 8\n","Saving model checkpoint to roberta-base-finetuned-classification/run-3/checkpoint-17\n","Configuration saved in roberta-base-finetuned-classification/run-3/checkpoint-17/config.json\n","Model weights saved in roberta-base-finetuned-classification/run-3/checkpoint-17/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/run-3/checkpoint-17/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/run-3/checkpoint-17/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from roberta-base-finetuned-classification/run-3/checkpoint-17 (score: 0.8571428571428572).\n","\u001b[32m[I 2022-06-21 01:54:01,611]\u001b[0m Trial 3 finished with value: 3.432601389838232 and parameters: {'learning_rate': 2.84776956713175e-05, 'num_train_epochs': 1, 'seed': 6, 'per_device_train_batch_size': 8}. Best is trial 3 with value: 3.432601389838232.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 132\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 85\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [85/85 01:28, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.523114</td>\n","      <td>0.878788</td>\n","      <td>0.928571</td>\n","      <td>0.812500</td>\n","      <td>0.866667</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.463035</td>\n","      <td>0.848485</td>\n","      <td>0.923077</td>\n","      <td>0.750000</td>\n","      <td>0.827586</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.682931</td>\n","      <td>0.757576</td>\n","      <td>0.900000</td>\n","      <td>0.562500</td>\n","      <td>0.692308</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.435951</td>\n","      <td>0.818182</td>\n","      <td>0.916667</td>\n","      <td>0.687500</td>\n","      <td>0.785714</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>0.254751</td>\n","      <td>0.878788</td>\n","      <td>0.928571</td>\n","      <td>0.812500</td>\n","      <td>0.866667</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 8\n","Saving model checkpoint to roberta-base-finetuned-classification/run-4/checkpoint-17\n","Configuration saved in roberta-base-finetuned-classification/run-4/checkpoint-17/config.json\n","Model weights saved in roberta-base-finetuned-classification/run-4/checkpoint-17/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/run-4/checkpoint-17/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/run-4/checkpoint-17/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 8\n","Saving model checkpoint to roberta-base-finetuned-classification/run-4/checkpoint-34\n","Configuration saved in roberta-base-finetuned-classification/run-4/checkpoint-34/config.json\n","Model weights saved in roberta-base-finetuned-classification/run-4/checkpoint-34/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/run-4/checkpoint-34/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/run-4/checkpoint-34/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 8\n","Saving model checkpoint to roberta-base-finetuned-classification/run-4/checkpoint-51\n","Configuration saved in roberta-base-finetuned-classification/run-4/checkpoint-51/config.json\n","Model weights saved in roberta-base-finetuned-classification/run-4/checkpoint-51/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/run-4/checkpoint-51/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/run-4/checkpoint-51/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 8\n","Saving model checkpoint to roberta-base-finetuned-classification/run-4/checkpoint-68\n","Configuration saved in roberta-base-finetuned-classification/run-4/checkpoint-68/config.json\n","Model weights saved in roberta-base-finetuned-classification/run-4/checkpoint-68/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/run-4/checkpoint-68/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/run-4/checkpoint-68/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 8\n","Saving model checkpoint to roberta-base-finetuned-classification/run-4/checkpoint-85\n","Configuration saved in roberta-base-finetuned-classification/run-4/checkpoint-85/config.json\n","Model weights saved in roberta-base-finetuned-classification/run-4/checkpoint-85/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/run-4/checkpoint-85/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/run-4/checkpoint-85/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from roberta-base-finetuned-classification/run-4/checkpoint-17 (score: 0.8666666666666666).\n","\u001b[32m[I 2022-06-21 01:55:30,230]\u001b[0m Trial 4 finished with value: 3.486525974025974 and parameters: {'learning_rate': 1.439009360014191e-05, 'num_train_epochs': 5, 'seed': 25, 'per_device_train_batch_size': 8}. Best is trial 4 with value: 3.486525974025974.\u001b[0m\n"]}]},{"cell_type":"markdown","source":["## Set the model with the best parameters and run it on the full dataset"],"metadata":{"id":"CDK2oaTH4uLJ"}},{"cell_type":"code","source":["for n, v in best_run.hyperparameters.items():\n","    setattr(trainer.args, n, v)\n","\n","trainer.train_dataset=train_dataset\n","trainer.eval_dataset=val_dataset\n","\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"neG3sjnaUHP_","executionInfo":{"status":"ok","timestamp":1655776757509,"user_tz":180,"elapsed":226464,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"33c38621-af53-44a7-858f-6462465d1daa"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 660\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 415\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='415' max='415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [415/415 03:43, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.444188</td>\n","      <td>0.843373</td>\n","      <td>0.882353</td>\n","      <td>0.692308</td>\n","      <td>0.775862</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.393227</td>\n","      <td>0.825301</td>\n","      <td>0.800000</td>\n","      <td>0.738462</td>\n","      <td>0.768000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.408810</td>\n","      <td>0.825301</td>\n","      <td>0.790323</td>\n","      <td>0.753846</td>\n","      <td>0.771654</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.513111</td>\n","      <td>0.837349</td>\n","      <td>0.851852</td>\n","      <td>0.707692</td>\n","      <td>0.773109</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>0.457833</td>\n","      <td>0.837349</td>\n","      <td>0.827586</td>\n","      <td>0.738462</td>\n","      <td>0.780488</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to roberta-base-finetuned-classification/checkpoint-83\n","Configuration saved in roberta-base-finetuned-classification/checkpoint-83/config.json\n","Model weights saved in roberta-base-finetuned-classification/checkpoint-83/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/checkpoint-83/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/checkpoint-83/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to roberta-base-finetuned-classification/checkpoint-166\n","Configuration saved in roberta-base-finetuned-classification/checkpoint-166/config.json\n","Model weights saved in roberta-base-finetuned-classification/checkpoint-166/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/checkpoint-166/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/checkpoint-166/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to roberta-base-finetuned-classification/checkpoint-249\n","Configuration saved in roberta-base-finetuned-classification/checkpoint-249/config.json\n","Model weights saved in roberta-base-finetuned-classification/checkpoint-249/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/checkpoint-249/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/checkpoint-249/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to roberta-base-finetuned-classification/checkpoint-332\n","Configuration saved in roberta-base-finetuned-classification/checkpoint-332/config.json\n","Model weights saved in roberta-base-finetuned-classification/checkpoint-332/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/checkpoint-332/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/checkpoint-332/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to roberta-base-finetuned-classification/checkpoint-415\n","Configuration saved in roberta-base-finetuned-classification/checkpoint-415/config.json\n","Model weights saved in roberta-base-finetuned-classification/checkpoint-415/pytorch_model.bin\n","tokenizer config file saved in roberta-base-finetuned-classification/checkpoint-415/tokenizer_config.json\n","Special tokens file saved in roberta-base-finetuned-classification/checkpoint-415/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from roberta-base-finetuned-classification/checkpoint-415 (score: 0.7804878048780489).\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=415, training_loss=0.3547350780073419, metrics={'train_runtime': 223.8652, 'train_samples_per_second': 14.741, 'train_steps_per_second': 1.854, 'total_flos': 741079009638000.0, 'train_loss': 0.3547350780073419, 'epoch': 5.0})"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["# ----- 3. Predict -----#\n","# Load test data\n","#test_data = pd.read_csv(\"test.csv\")\n","test = pd.read_csv(\"./input/data_test.csv\")\n","\n","sequence_formatted = []\n","for seq in test['sequence'].values:\n","  sequence_formatted.append(\" \".join(seq))\n","\n","test_data = pd.DataFrame({'sequence':sequence_formatted, 'label':test['label'].tolist()})\n","\n","\n","X_test = list(test_data[\"sequence\"])\n","X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n","\n","# Create torch dataset\n","test_dataset = Dataset(X_test_tokenized)\n","\n","# Make prediction\n","raw_pred, _, _ = trainer.predict(test_dataset)\n","\n","# Preprocess raw predictions\n","y_pred = np.argmax(raw_pred, axis=1)"],"metadata":{"id":"Jvyc17u1IB2H","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1655776758157,"user_tz":180,"elapsed":683,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"459ddd39-c479-4024-9f89-fc0669cb217f"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 207\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [26/26 00:02]\n","    </div>\n","    "]},"metadata":{}}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.metrics import roc_auc_score\n","\n","print(\"ROC_AUC:\", roc_auc_score(test_data['label'], y_pred))\n","\n","print(classification_report(test_data['label'], y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pda1MPDHzFnc","executionInfo":{"status":"ok","timestamp":1655776758161,"user_tz":180,"elapsed":41,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"a8561ec2-3b0a-4f5a-aacf-0c6c0d7894fe"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["ROC_AUC: 0.8443468296409472\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.92      0.88       119\n","           1       0.87      0.77      0.82        88\n","\n","    accuracy                           0.86       207\n","   macro avg       0.86      0.84      0.85       207\n","weighted avg       0.86      0.86      0.85       207\n","\n"]}]}],"metadata":{"colab":{"name":"epitope_prediction_optimized_roberta_uncased.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}