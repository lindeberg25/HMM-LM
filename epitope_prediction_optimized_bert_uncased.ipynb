{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/refine-epitope-deep-learning')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLxgv9OSFqYi","executionInfo":{"status":"ok","timestamp":1657144030909,"user_tz":180,"elapsed":30152,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"4e6cd4b0-98e3-4123-f07d-def901db77e4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install optuna"],"metadata":{"id":"Eu47NwKQudlO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657144047401,"user_tz":180,"elapsed":16520,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"3adfc18d-4a8e-4005-8379-94a7c82fd3cc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 7.6 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 11.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 66.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 71.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting optuna\n","  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n","\u001b[K     |████████████████████████████████| 308 kB 9.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n","Collecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Collecting cliff\n","  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 9.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n","Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n","Collecting alembic\n","  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 56.4 MB/s \n","\u001b[?25hCollecting colorlog\n","  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.37)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.4)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.7.1)\n","Collecting Mako\n","  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 8.4 MB/s \n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n","Collecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.9.0-py2.py3-none-any.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 67.2 MB/s \n","\u001b[?25hCollecting autopage>=0.4.0\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Collecting cmd2>=1.0.0\n","  Downloading cmd2-2.4.1-py3-none-any.whl (146 kB)\n","\u001b[K     |████████████████████████████████| 146 kB 75.7 MB/s \n","\u001b[?25hCollecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 6.8 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n","Building wheels for collected packages: pyperclip\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=e32749e206caf2e8435fc22bb16bae8c3381f8d9cdd30a93c5d51201e2dd7047\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built pyperclip\n","Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n","Successfully installed Mako-1.2.1 alembic-1.8.0 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.1 colorlog-6.6.0 optuna-2.10.1 pbr-5.9.0 pyperclip-1.8.2 stevedore-3.5.0\n"]}]},{"cell_type":"markdown","source":["# Preprocess data "],"metadata":{"id":"QEpSHqtISuKr"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","import torch\n","from transformers import TrainingArguments, Trainer\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import EarlyStoppingCallback"],"metadata":{"id":"eNWYJ8r-c_w3","executionInfo":{"status":"ok","timestamp":1657144053658,"user_tz":180,"elapsed":6275,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Create torch dataset\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels:\n","            item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])"],"metadata":{"id":"aj8NS4koSdbh","executionInfo":{"status":"ok","timestamp":1657144053658,"user_tz":180,"elapsed":8,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def preprocess_data(data):\n","\n","    # Preprocess data\n","    X = list(data[\"sequence\"])\n","    y = list(data[\"label\"])\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n","    X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n","    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n","\n","    train_dataset = Dataset(X_train_tokenized, y_train)\n","    val_dataset = Dataset(X_val_tokenized, y_val)\n","    return train_dataset, val_dataset"],"metadata":{"id":"3WcVTolFqPS9","executionInfo":{"status":"ok","timestamp":1657144053659,"user_tz":180,"elapsed":7,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv(\"./input/data_train.csv\")\n","\n","sequence_formatted = []\n","for seq in df_train['sequence'].values:\n","  sequence_formatted.append(\" \".join(seq))\n","\n","data = pd.DataFrame({'sequence':sequence_formatted, 'label':df_train['label'].tolist()})\n","\n","#data = df_train\n","\n","data_op = data[:int(len(data)/2)]\n","\n","\n","# Define pretrained tokenizer and model\n","batch_size=8\n","model_name = \"bert-base-uncased\"\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","train_dataset_op, val_dataset_op = preprocess_data(data_op)\n","train_dataset, val_dataset = preprocess_data(data)\n","\n","# ----- 2. Fine-tune pretrained model -----#\n","# Define Trainer parameters\n","def compute_metrics(p):\n","    \n","    pred, labels = p\n","    pred = np.argmax(pred, axis=1)\n","\n","    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","    recall = recall_score(y_true=labels, y_pred=pred)\n","    precision = precision_score(y_true=labels, y_pred=pred)\n","    f1 = f1_score(y_true=labels, y_pred=pred)\n","\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","# Define Trainer\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-classification\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=1,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model='f1',\n","    optim=\"adamw_torch\"\n",")\n","\n","def model_init():\n","    return model\n","\n","trainer = Trainer(\n","    model_init=model_init,\n","    args=args,\n","    train_dataset=train_dataset_op,\n","    eval_dataset=val_dataset_op,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","\n","best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")\n","\n"],"metadata":{"id":"61S6rdO_FPMP","executionInfo":{"status":"ok","timestamp":1657144483661,"user_tz":180,"elapsed":430008,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8c840b5488cb49e4aba32cbdd83ed1f1","c82da35afa4941338d2bc9bcd4e21b92","ab3ff218b6fb45cf893cc27298adc976","3c1ad70005e94d82be2d9351b5090dd8","f0f65223c00048b7a2dc1cf760e74efe","8595daf45e4642b6b54d70c64f12e429","ad003e9419904e7ba4751f817af90b01","1d6ba31dd67a43e0a8a16c16c8fb90d5","00e64bac56d14524ba4fd4d08a623f3e","18d2daccbd0d41f189e12ed0a7084df9","86a756daf0d6405ab408775fd707866e","010eda2ff8ea427e8768886f7a41d709","fa174cbc9321484d9c25387fa308e761","25e3916e700649839d4e1b9ff8dd52b8","89f8fc4f001f4209838d5efa137701c8","d40c6abd1ee84f3ab13872c67a5434fd","d6f1682ab6c34f73830665fce5169987","b9217de73742474a90f16e0ccfb2d6fe","e49a90b62d554043b55de2f11ca62b69","bed7f63939ed4f34a3d5dd030a34fd69","d28402f67b6f46fabd87f4423fb618f7","cb6ab63f9a9d4be3b4509f4244ed9fcb","e5d668c92ac74f05b04f55465092a872","be30c30b1ee04b1f9a6aa3aaebbd77c8","06e4ec5fe7a144419098b590b9db98a8","527a80245bdd48d2bbdf32d3febe97ad","7d6feb0dc0be4637a858757a9a49673d","2c9688b9dd98402889feeaa4f56aa8f7","2f26166ff26d42119a2c318e2b0c6b90","30b6b9b021e94739938b0cc02fa96ca9","7acaccddee62492abf24ce1e046e381a","2064c5e832ec4b6db65d900186820360","69f36739f45b47419bdb69ce041f7133","581030c217ad48caadbf722ff33fa19e","d6e7a6d1aebe4054b4b75dba31f5e70c","fa32d05f35a74d4ea2f01485a8ba469f","38354419b13a48eba0563b0be9fc65f1","6051b6252d1f4784871750ed13e982d0","981715451b8544909a8b44525d4e63c8","313f8120b45743c3972d39826f59786b","68b31e6e8c414c59bc5f8bbf55c409a0","4fb8ace224544e65abe9c2594f54cd15","7941a52b5d504d6ab10989fb73f2c5ac","d893cc6f3f234d8782d73b732a457174"]},"outputId":"a17797e3-8a8e-4c90-ad23-952e015cbdff"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c840b5488cb49e4aba32cbdd83ed1f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"010eda2ff8ea427e8768886f7a41d709"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5d668c92ac74f05b04f55465092a872"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"581030c217ad48caadbf722ff33fa19e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m[I 2022-07-06 21:48:02,427]\u001b[0m A new study created in memory with name: no-name-d18692a7-34c9-4d68-933a-0a34ad864907\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 210\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [210/210 02:01, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.546304</td>\n","      <td>0.614458</td>\n","      <td>0.415094</td>\n","      <td>0.956522</td>\n","      <td>0.578947</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.359152</td>\n","      <td>0.843373</td>\n","      <td>0.678571</td>\n","      <td>0.826087</td>\n","      <td>0.745098</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.360683</td>\n","      <td>0.831325</td>\n","      <td>0.695652</td>\n","      <td>0.695652</td>\n","      <td>0.695652</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.368075</td>\n","      <td>0.843373</td>\n","      <td>0.692308</td>\n","      <td>0.782609</td>\n","      <td>0.734694</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>0.369946</td>\n","      <td>0.843373</td>\n","      <td>0.708333</td>\n","      <td>0.739130</td>\n","      <td>0.723404</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-0/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-0/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-0/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-126/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-0/checkpoint-168\n","Configuration saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-168/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-168/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-168/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-168/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-0/checkpoint-210\n","Configuration saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-210/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-210/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-210/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-0/checkpoint-210/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-0/checkpoint-84 (score: 0.7450980392156864).\n","\u001b[32m[I 2022-07-06 21:50:04,255]\u001b[0m Trial 0 finished with value: 3.0142415174109947 and parameters: {'learning_rate': 1.4174983413882713e-05, 'num_train_epochs': 5, 'seed': 26, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 3.0142415174109947.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 42\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [42/42 00:23, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.368284</td>\n","      <td>0.831325</td>\n","      <td>0.680000</td>\n","      <td>0.739130</td>\n","      <td>0.708333</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-1/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-1/checkpoint-42/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-1/checkpoint-42 (score: 0.7083333333333334).\n","\u001b[32m[I 2022-07-06 21:50:28,001]\u001b[0m Trial 1 finished with value: 2.9587890693207615 and parameters: {'learning_rate': 5.598627372287537e-06, 'num_train_epochs': 1, 'seed': 21, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 3.0142415174109947.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 42\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [42/42 00:23, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.364647</td>\n","      <td>0.843373</td>\n","      <td>0.708333</td>\n","      <td>0.739130</td>\n","      <td>0.723404</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-2/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-2/checkpoint-42/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-2/checkpoint-42 (score: 0.723404255319149).\n","\u001b[32m[I 2022-07-06 21:50:52,166]\u001b[0m Trial 2 finished with value: 3.0142415174109947 and parameters: {'learning_rate': 6.157575481226498e-06, 'num_train_epochs': 1, 'seed': 1, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 3.0142415174109947.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 168\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [168/168 01:35, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.368660</td>\n","      <td>0.867470</td>\n","      <td>0.700000</td>\n","      <td>0.913043</td>\n","      <td>0.792453</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.390878</td>\n","      <td>0.843373</td>\n","      <td>0.708333</td>\n","      <td>0.739130</td>\n","      <td>0.723404</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.388486</td>\n","      <td>0.843373</td>\n","      <td>0.708333</td>\n","      <td>0.739130</td>\n","      <td>0.723404</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.372493</td>\n","      <td>0.843373</td>\n","      <td>0.708333</td>\n","      <td>0.739130</td>\n","      <td>0.723404</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-3/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-3/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-84/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-3/checkpoint-126\n","Configuration saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-126/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-126/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-126/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-126/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-3/checkpoint-168\n","Configuration saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-168/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-168/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-168/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-3/checkpoint-168/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-3/checkpoint-42 (score: 0.7924528301886793).\n","\u001b[32m[I 2022-07-06 21:52:28,408]\u001b[0m Trial 3 finished with value: 3.0142415174109947 and parameters: {'learning_rate': 1.0834415013219388e-05, 'num_train_epochs': 4, 'seed': 30, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 3.0142415174109947.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 84\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [84/84 00:47, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.366602</td>\n","      <td>0.843373</td>\n","      <td>0.666667</td>\n","      <td>0.869565</td>\n","      <td>0.754717</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.365615</td>\n","      <td>0.867470</td>\n","      <td>0.772727</td>\n","      <td>0.739130</td>\n","      <td>0.755556</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-4/checkpoint-42\n","Configuration saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-42/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-42/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-42/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-42/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/run-4/checkpoint-84\n","Configuration saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-84/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-84/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-84/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/run-4/checkpoint-84/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/run-4/checkpoint-84 (score: 0.7555555555555555).\n","\u001b[32m[I 2022-07-06 21:53:16,130]\u001b[0m Trial 4 finished with value: 3.134883142583509 and parameters: {'learning_rate': 6.337162035692807e-06, 'num_train_epochs': 2, 'seed': 6, 'per_device_train_batch_size': 16}. Best is trial 4 with value: 3.134883142583509.\u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 42\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [42/42 00:17, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.449832</td>\n","      <td>0.807229</td>\n","      <td>0.652174</td>\n","      <td>0.652174</td>\n","      <td>0.652174</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","\u001b[32m[I 2022-07-06 21:53:34,062]\u001b[0m Trial 5 pruned. \u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 168\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 42/168 00:16 < 00:52, 2.39 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.558187</td>\n","      <td>0.662651</td>\n","      <td>0.448980</td>\n","      <td>0.956522</td>\n","      <td>0.611111</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","\u001b[32m[I 2022-07-06 21:53:51,299]\u001b[0m Trial 6 pruned. \u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 42\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [42/42 00:16, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.556754</td>\n","      <td>0.674699</td>\n","      <td>0.458333</td>\n","      <td>0.956522</td>\n","      <td>0.619718</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","\u001b[32m[I 2022-07-06 21:54:08,532]\u001b[0m Trial 7 pruned. \u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 126\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 42/126 00:16 < 00:35, 2.39 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.448385</td>\n","      <td>0.746988</td>\n","      <td>0.523810</td>\n","      <td>0.956522</td>\n","      <td>0.676923</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","\u001b[32m[I 2022-07-06 21:54:25,724]\u001b[0m Trial 8 pruned. \u001b[0m\n","Trial:\n","***** Running training *****\n","  Num examples = 330\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 84\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [42/84 00:16 < 00:17, 2.39 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.387240</td>\n","      <td>0.831325</td>\n","      <td>0.680000</td>\n","      <td>0.739130</td>\n","      <td>0.708333</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 83\n","  Batch size = 8\n","\u001b[32m[I 2022-07-06 21:54:42,918]\u001b[0m Trial 9 pruned. \u001b[0m\n"]}]},{"cell_type":"markdown","source":["## Set the model with the best parameters and run it on the full dataset"],"metadata":{"id":"CDK2oaTH4uLJ"}},{"cell_type":"code","source":["for n, v in best_run.hyperparameters.items():\n","    setattr(trainer.args, n, v)\n","\n","trainer.train_dataset=train_dataset\n","trainer.eval_dataset=val_dataset\n","\n","trainer.train()"],"metadata":{"id":"neG3sjnaUHP_","executionInfo":{"status":"ok","timestamp":1657144575554,"user_tz":180,"elapsed":91921,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"colab":{"base_uri":"https://localhost:8080/","height":664},"outputId":"343c3159-4eda-4782-dc3a-21a8a56ea403"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 660\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 166\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='166' max='166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [166/166 01:31, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.283431</td>\n","      <td>0.867470</td>\n","      <td>0.854167</td>\n","      <td>0.732143</td>\n","      <td>0.788462</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.272089</td>\n","      <td>0.879518</td>\n","      <td>0.860000</td>\n","      <td>0.767857</td>\n","      <td>0.811321</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/checkpoint-83\n","Configuration saved in bert-base-uncased-finetuned-classification/checkpoint-83/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/checkpoint-83/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/checkpoint-83/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/checkpoint-83/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 166\n","  Batch size = 8\n","Saving model checkpoint to bert-base-uncased-finetuned-classification/checkpoint-166\n","Configuration saved in bert-base-uncased-finetuned-classification/checkpoint-166/config.json\n","Model weights saved in bert-base-uncased-finetuned-classification/checkpoint-166/pytorch_model.bin\n","tokenizer config file saved in bert-base-uncased-finetuned-classification/checkpoint-166/tokenizer_config.json\n","Special tokens file saved in bert-base-uncased-finetuned-classification/checkpoint-166/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from bert-base-uncased-finetuned-classification/checkpoint-166 (score: 0.8113207547169812).\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=166, training_loss=0.32443432635571584, metrics={'train_runtime': 91.9071, 'train_samples_per_second': 14.362, 'train_steps_per_second': 1.806, 'total_flos': 655948194343200.0, 'train_loss': 0.32443432635571584, 'epoch': 2.0})"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# ----- 3. Predict -----#\n","# Load test data\n","#test_data = pd.read_csv(\"test.csv\")\n","test = pd.read_csv(\"./input/data_test.csv\")\n","\n","sequence_formatted = []\n","for seq in test['sequence'].values:\n","  sequence_formatted.append(\" \".join(seq))\n","\n","test_data = pd.DataFrame({'sequence':sequence_formatted, 'label':test['label'].tolist()})\n","\n","\n","X_test = list(test_data[\"sequence\"])\n","X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n","\n","# Create torch dataset\n","test_dataset = Dataset(X_test_tokenized)\n","\n","# Make prediction\n","raw_pred, _, _ = trainer.predict(test_dataset)\n","\n","# Preprocess raw predictions\n","y_pred = np.argmax(raw_pred, axis=1)\n"],"metadata":{"id":"Jvyc17u1IB2H","executionInfo":{"status":"ok","timestamp":1657144582386,"user_tz":180,"elapsed":6845,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"460c04aa-8583-4f20-db98-fe0a5eba74e3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 207\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [26/26 00:02]\n","    </div>\n","    "]},"metadata":{}}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.metrics import roc_auc_score\n","\n","print(\"ROC_AUC:\", roc_auc_score(test_data['label'], y_pred))\n","\n","print(classification_report(test_data['label'], y_pred))"],"metadata":{"id":"ieo_rZQnMC1i","executionInfo":{"status":"ok","timestamp":1657144582389,"user_tz":180,"elapsed":44,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1d7e78b7-dd45-42f0-8b1c-347113ffbf71"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["ROC_AUC: 0.8216195569136746\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.92      0.87       119\n","           1       0.86      0.73      0.79        88\n","\n","    accuracy                           0.84       207\n","   macro avg       0.84      0.82      0.83       207\n","weighted avg       0.84      0.84      0.83       207\n","\n"]}]}],"metadata":{"colab":{"name":"epitope_prediction_optimized_bert_uncased.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8c840b5488cb49e4aba32cbdd83ed1f1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c82da35afa4941338d2bc9bcd4e21b92","IPY_MODEL_ab3ff218b6fb45cf893cc27298adc976","IPY_MODEL_3c1ad70005e94d82be2d9351b5090dd8"],"layout":"IPY_MODEL_f0f65223c00048b7a2dc1cf760e74efe"}},"c82da35afa4941338d2bc9bcd4e21b92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8595daf45e4642b6b54d70c64f12e429","placeholder":"​","style":"IPY_MODEL_ad003e9419904e7ba4751f817af90b01","value":"Downloading: 100%"}},"ab3ff218b6fb45cf893cc27298adc976":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d6ba31dd67a43e0a8a16c16c8fb90d5","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00e64bac56d14524ba4fd4d08a623f3e","value":231508}},"3c1ad70005e94d82be2d9351b5090dd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18d2daccbd0d41f189e12ed0a7084df9","placeholder":"​","style":"IPY_MODEL_86a756daf0d6405ab408775fd707866e","value":" 226k/226k [00:00&lt;00:00, 887kB/s]"}},"f0f65223c00048b7a2dc1cf760e74efe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8595daf45e4642b6b54d70c64f12e429":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad003e9419904e7ba4751f817af90b01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d6ba31dd67a43e0a8a16c16c8fb90d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00e64bac56d14524ba4fd4d08a623f3e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"18d2daccbd0d41f189e12ed0a7084df9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86a756daf0d6405ab408775fd707866e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"010eda2ff8ea427e8768886f7a41d709":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa174cbc9321484d9c25387fa308e761","IPY_MODEL_25e3916e700649839d4e1b9ff8dd52b8","IPY_MODEL_89f8fc4f001f4209838d5efa137701c8"],"layout":"IPY_MODEL_d40c6abd1ee84f3ab13872c67a5434fd"}},"fa174cbc9321484d9c25387fa308e761":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6f1682ab6c34f73830665fce5169987","placeholder":"​","style":"IPY_MODEL_b9217de73742474a90f16e0ccfb2d6fe","value":"Downloading: 100%"}},"25e3916e700649839d4e1b9ff8dd52b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e49a90b62d554043b55de2f11ca62b69","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bed7f63939ed4f34a3d5dd030a34fd69","value":28}},"89f8fc4f001f4209838d5efa137701c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d28402f67b6f46fabd87f4423fb618f7","placeholder":"​","style":"IPY_MODEL_cb6ab63f9a9d4be3b4509f4244ed9fcb","value":" 28.0/28.0 [00:00&lt;00:00, 813B/s]"}},"d40c6abd1ee84f3ab13872c67a5434fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6f1682ab6c34f73830665fce5169987":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9217de73742474a90f16e0ccfb2d6fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e49a90b62d554043b55de2f11ca62b69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bed7f63939ed4f34a3d5dd030a34fd69":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d28402f67b6f46fabd87f4423fb618f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb6ab63f9a9d4be3b4509f4244ed9fcb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5d668c92ac74f05b04f55465092a872":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be30c30b1ee04b1f9a6aa3aaebbd77c8","IPY_MODEL_06e4ec5fe7a144419098b590b9db98a8","IPY_MODEL_527a80245bdd48d2bbdf32d3febe97ad"],"layout":"IPY_MODEL_7d6feb0dc0be4637a858757a9a49673d"}},"be30c30b1ee04b1f9a6aa3aaebbd77c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c9688b9dd98402889feeaa4f56aa8f7","placeholder":"​","style":"IPY_MODEL_2f26166ff26d42119a2c318e2b0c6b90","value":"Downloading: 100%"}},"06e4ec5fe7a144419098b590b9db98a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_30b6b9b021e94739938b0cc02fa96ca9","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7acaccddee62492abf24ce1e046e381a","value":570}},"527a80245bdd48d2bbdf32d3febe97ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2064c5e832ec4b6db65d900186820360","placeholder":"​","style":"IPY_MODEL_69f36739f45b47419bdb69ce041f7133","value":" 570/570 [00:00&lt;00:00, 14.8kB/s]"}},"7d6feb0dc0be4637a858757a9a49673d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c9688b9dd98402889feeaa4f56aa8f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f26166ff26d42119a2c318e2b0c6b90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30b6b9b021e94739938b0cc02fa96ca9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7acaccddee62492abf24ce1e046e381a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2064c5e832ec4b6db65d900186820360":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69f36739f45b47419bdb69ce041f7133":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"581030c217ad48caadbf722ff33fa19e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6e7a6d1aebe4054b4b75dba31f5e70c","IPY_MODEL_fa32d05f35a74d4ea2f01485a8ba469f","IPY_MODEL_38354419b13a48eba0563b0be9fc65f1"],"layout":"IPY_MODEL_6051b6252d1f4784871750ed13e982d0"}},"d6e7a6d1aebe4054b4b75dba31f5e70c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_981715451b8544909a8b44525d4e63c8","placeholder":"​","style":"IPY_MODEL_313f8120b45743c3972d39826f59786b","value":"Downloading: 100%"}},"fa32d05f35a74d4ea2f01485a8ba469f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_68b31e6e8c414c59bc5f8bbf55c409a0","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4fb8ace224544e65abe9c2594f54cd15","value":440473133}},"38354419b13a48eba0563b0be9fc65f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7941a52b5d504d6ab10989fb73f2c5ac","placeholder":"​","style":"IPY_MODEL_d893cc6f3f234d8782d73b732a457174","value":" 420M/420M [00:07&lt;00:00, 61.8MB/s]"}},"6051b6252d1f4784871750ed13e982d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"981715451b8544909a8b44525d4e63c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"313f8120b45743c3972d39826f59786b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68b31e6e8c414c59bc5f8bbf55c409a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fb8ace224544e65abe9c2594f54cd15":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7941a52b5d504d6ab10989fb73f2c5ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d893cc6f3f234d8782d73b732a457174":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}